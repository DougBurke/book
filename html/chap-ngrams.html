<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Chapter 2. N-grams</title>
<link rel="stylesheet" href="screen.css" type="text/css">
<meta name="generator" content="DocBook XSL Stylesheets V1.75.2">
<link rel="home" href="index.html" title="Natural Language Processing for the Working Programmer">
<link rel="up" href="index.html" title="Natural Language Processing for the Working Programmer">
<link rel="prev" href="chap-words.html" title="Chapter 1. Words">
</head>
<body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF">
<div class="navheader">
<table width="100%" summary="Navigation header">
<tr><th colspan="3" align="center">Chapter 2. N-grams</th></tr>
<tr>
<td width="20%" align="left">
<a accesskey="p" href="chap-words.html">Prev</a> </td>
<th width="60%" align="center"> </th>
<td width="20%" align="right"> </td>
</tr>
</table>
<hr>
</div>
<div class="chapter" title="Chapter 2. N-grams">
<div class="titlepage"><div><div><h2 class="title">
<a name="chap-ngrams"></a>Chapter 2. N-grams</h2></div></div></div>
<div class="toc">
<p><b>Table of Contents</b></p>
<dl>
<dt><span class="sect1"><a href="chap-ngrams.html#id36914643">2.1. Introduction</a></span></dt>
<dt><span class="sect1"><a href="chap-ngrams.html#id36914773">2.2. Bigrams</a></span></dt>
<dt><span class="sect1"><a href="chap-ngrams.html#id36914774">2.3. From bigrams to n-grams</a></span></dt>
<dt><span class="sect1"><a href="chap-ngrams.html#id36914974">2.4. Collocations</a></span></dt>
</dl>
</div>
    
    <div class="sect1" title="2.1. Introduction">
<div class="titlepage"><div><div><h2 class="title" style="clear: both">
<a name="id36914643"></a>2.1. Introduction</h2></div></div></div>
        
        <p>In the previous chapter, we have looked at words, and the combination of words into a
            higher level of meaning representation: a sentence. As you might recall being told by
            your high school grammar teacher, not every random combination of words forms an
            grammatically acceptable sentence:</p>
        <div class="itemizedlist"><ul class="itemizedlist" type="disc">
<li class="listitem">
                <p>Colorless green ideas sleep furiously</p>
            </li>
<li class="listitem">
                <p>Furiously sleep ideas green colorless</p>
            </li>
<li class="listitem">
                <p>Ideas furiously colorless sleep green</p>
            </li>
</ul></div>
        <p>The sentence <span class="italic">Colorless green ideas sleep furiously</span>
            (made famous by the linguist Noam Chomsky), for instance, is grammatically perfectly
            acceptable, but of course entirely non-sensical (unless you ate wrong/weird mushrooms,
            that is). If you compare this sentence to the other two sentences, this grammaticality
            becomes evident. The sentence <span class="italic">Furiously sleep ideas green
                colorless</span> is grammatically unacceptable, and so is <span class="italic">Ideas furiously colorless sleep green</span>: these sentences do
            not play by the rules of the english language. In other words, the fact that languages
            have rules constraints the way in which words can be combined into an acceptable
            sentences.</p>
        <p>Hey! That sounds good for us NLP programmers (we can almost here you think), language
            plays by rules, computers work with rules, well, we’re done, aren’t we? We’ll infer a
            set of rules, and there! we have ourselves <span class="italic">language
                model</span>. A model that describes how a language, say English, works and
            behaves. Well, not so fast buster! Although we will certainly discuss our share of such
            rule-based language models later on (in the chapter about parsing), the fact is that
            nature is simply not so simple. The rules by which a language plays are very complex,
            and no full set of rules to describe a language has ever been proposed. Bummer, isn’t
            it? Lucky for us, there are simpler ways to obtain a language model, namely by
            exploiting the observation that words do not combine in a random order. That is, we can
            learn a lot from a word and its neighbors. Language models that exploit the ordering of
            words, are called <span class="italic">n-gram language models</span>, in which
            the <span class="italic">n</span> represents any integer greater than
            zero.</p>
        <p>N-gram models can be imagined as placing a small window over a sentence or a text, in
            which only <span class="italic">n</span> words are visible at the same time. The
            simplest n-gram model is therefore a so-called <span class="italic">unigram</span> model. This is a model in which we only look at one word at a
            time. The sentence <span class="italic">Colorless green ideas sleep
                furiously</span>, for instance, contains five unigrams: “colorless”, “green”,
            “ideas”, “sleep”, and “furiously”. Of course, this is not very informative, as these are
            just the words that form the sentence. In fact, N-grams start to become interesting when
                <span class="italic">n</span> is two (a <span class="italic">bigram</span>) or greater. Let us start with bigrams.</p>
    </div>
    <div class="sect1" title="2.2. Bigrams">
<div class="titlepage"><div><div><h2 class="title" style="clear: both">
<a name="id36914773"></a>2.2. Bigrams</h2></div></div></div>
        
        <p>An unigram can be thought of as a window placed over a text, such that we only look at
            one word at a time. In similar fashion, a bigram can be thought of as a window that
            shows two words at a time. The sentence <span class="italic">Colorless green ideas
                sleep furiously</span> contains four bigrams:</p>
<div class="itemizedlist"><ul class="itemizedlist" type="disc">
<li class="listitem">
                    <p>Colorless, green</p>
                </li>
<li class="listitem">
                    <p>green, ideas</p>
                </li>
<li class="listitem">
                    <p>ideas, sleep</p>
                </li>
<li class="listitem">
                    <p>sleep, furiously</p>
                </li>
</ul></div>
        <p>To stick to our ‘window’ analogy, we could say that all bigrams of a sentence can be
            found by placing a window on its first two words, and by moving this window to the right
            one word at a time in a stepwise manner. We then repeat this procedure, until the window
            covers the last two words of a sentence. In fact, the same holds for unigrams and
            N-grams with <span class="italic">n</span> greater than two. So, say we we have a
            body of text represented as a list of words or tokens (whatever you prefer). For the
            sake of legacy, we will stick to a list of tokens representing the sentence <span class="italic">Colorless green ideas sleep furiously</span>:</p>
        <pre class="screen">Prelude&gt; <span class="command"><strong>["Colorless", "green", "ideas", "sleep", "furiously"]</strong></span>
["Colorless","green","ideas","sleep","furiously"]</pre>
        <p>Hey! That looks like… indeed, that looks like a list of unigrams! Well, that was
            convenient. Unfortunately, things do not remain so simple if we move from unigrams to
            bigrams or <span class="italic">some-very-large-n-grams</span>. Bigrams and n-gram require
				us to construct 'windows' that cover more than one word at a time. In case of bigrams, for instance,
				this means that we would like to obtain a list of lists of two words (bigrams).
				Represented in such a way, the list of bigrams in the sentence <span class="italic">Colorless green ideas sleep
                furiously</span> would look like this:</p>
	<pre class="screen"><span class="command"><strong>[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</strong></span></pre>
		<p>To arrive at such a list, we could start out with a list of words (yes indeed, the unigrams),
			and complete the following sequence of steps:
			</p>
<div class="orderedlist"><ol class="orderedlist" type="1">
<li class="listitem">
		 			<p>Place a window on the first bigram, and add it to our bigram list</p>
				</li>
<li class="listitem">
					<p>Move the window one word to the right</p>
				</li>
<li class="listitem">
					<p>Repeat from the first step, until the last bigram is stored</p>
				</li>
</ol></div>
<p>
			Provided these steps, we first need a way to place a window on the first bigram, that is, we need to
			isolate the first two items of the list of words. In the prelude, Haskell defines
			a function named <span class="italic">take</span> that seems to suit our needs:
		</p> 
		<pre class="screen">Prelude&gt; <span class="command"><strong>:type take</strong></span> 
take :: Int -&gt; [a] -&gt; [a]</pre>
		<p>
		This function takes an <span class="italic">Integer</span> denoting <span class="italic">n</span> number of elements, 
		and a list of some type <span class="italic">a</span>. Given these arguments, it returns the first
		<span class="italic">n</span> elements of a list of <span class="italic">a</span>s.
		Thus, passing it the number two and a list of words should give us... our first bigram:</p>
        <pre class="screen">Prelude&gt; <span class="command"><strong>take 2 ["Colorless", "green", "ideas", "sleep", "furiously"]</strong></span>
["Colorless","green"]</pre>
		<p>Great! That worked out nice! Now from here on off, the idea is to add this bigram to a list,
			and to move the window one word to the right, so that we obtain the second bigram. Let us first
			turn to the latter (as we will get the list part for free later on). How do we move the window
			one word to the right?
		</p>
    </div>
    <div class="sect1" title="2.3. From bigrams to n-grams">
<div class="titlepage"><div><div><h2 class="title" style="clear: both">
<a name="id36914774"></a>2.3. From bigrams to n-grams</h2></div></div></div>
        
        <p>Stub</p>
    </div>
    <div class="sect1" title="2.4. Collocations">
<div class="titlepage"><div><div><h2 class="title" style="clear: both">
<a name="id36914974"></a>2.4. Collocations</h2></div></div></div>
        
        <p>Stub</p>
    </div>
</div>
<div class="navfooter">
<hr>
<table width="100%" summary="Navigation footer">
<tr>
<td width="40%" align="left">
<a accesskey="p" href="chap-words.html">Prev</a> </td>
<td width="20%" align="center"> </td>
<td width="40%" align="right"> </td>
</tr>
<tr>
<td width="40%" align="left" valign="top">Chapter 1. Words </td>
<td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td>
<td width="40%" align="right" valign="top"> </td>
</tr>
</table>
</div>
</body>
</html>
