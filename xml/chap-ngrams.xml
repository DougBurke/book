<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbookxi.rng" type="xml"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="chap-ngrams">
    <title>N-grams</title>
    <sect1 xml:id="chap-ngrams-intro">
        <title>Introduction</title>
        <para>In the previous chapter, we have looked at words, and the combination of words into a
            higher level of meaning representation: a sentence. As you might recall being told by
            your high school grammar teacher, not every random combination of words forms an
            grammatically acceptable sentence:</para>
        <itemizedlist>
            <listitem>
                <para>Colorless green ideas sleep furiously</para>
            </listitem>
            <listitem>
                <para>Furiously sleep ideas green colorless</para>
            </listitem>
            <listitem>
                <para>Ideas furiously colorless sleep green</para>
            </listitem>
        </itemizedlist>
        <para>The sentence <emphasis role="italic">Colorless green ideas sleep furiously</emphasis>
            (made famous by the linguist Noam Chomsky), for instance, is grammatically perfectly
            acceptable, but of course entirely nonsensical (unless you ate wrong/weird mushrooms,
            that is). If you compare this sentence to the other two sentences, this grammaticality
            becomes evident. The sentence <emphasis role="italic">Furiously sleep ideas green
                colorless</emphasis> is grammatically unacceptable, and so is <emphasis
                role="italic">Ideas furiously colorless sleep green</emphasis>: these sentences do
            not play by the rules of the English language. In other words, the fact that languages
            have rules constraints the way in which words can be combined into an acceptable
            sentences.</para>
        <para>Hey! That sounds good for us NLP programmers (we can almost hear you think), language
            plays by rules, computers work with rules, well, we’re done, aren’t we? We’ll infer a
            set of rules, and there! we have ourselves <emphasis role="italic">language
                model</emphasis>. A model that describes how a language, say English, works and
            behaves. Well, not so fast buster! Although we will certainly discuss our share of such
            rule-based language models later on (in the chapter about parsing), the fact is that
            nature is simply not so simple. The rules by which a language plays are very complex,
            and no full set of rules to describe a language has ever been proposed. Bummer, isn’t
            it? Lucky for us, there are simpler ways to obtain a language model, namely by
            exploiting the observation that words do not combine in a random order. That is, we can
            learn a lot from a word and its neighbors. Language models that exploit the ordering of
            words, are called <emphasis role="italic">n-gram language models</emphasis>, in which
            the <emphasis role="italic">n</emphasis> represents any integer greater than
            zero.</para>
        <para>N-gram models can be imagined as placing a small window over a sentence or a text, in
            which only <emphasis role="italic">n</emphasis> words are visible at the same time. The
            simplest n-gram model is therefore a so-called <emphasis role="italic"
                >unigram</emphasis> model. This is a model in which we only look at one word at a
            time. The sentence <emphasis role="italic">Colorless green ideas sleep
                furiously</emphasis>, for instance, contains five unigrams: “colorless”, “green”,
            “ideas”, “sleep”, and “furiously”. Of course, this is not very informative, as these are
            just the words that form the sentence. In fact, N-grams start to become interesting when
                <emphasis role="italic">n</emphasis> is two (a <emphasis role="italic"
                >bigram</emphasis>) or greater. Let us start with bigrams.</para>
    </sect1>
    <sect1 xml:id="chap-ngrams-bigrams">
        <title>Bigrams</title>
        <para>An unigram can be thought of as a window placed over a text, such that we only look at
            one word at a time. In similar fashion, a bigram can be thought of as a window that
            shows two words at a time. The sentence <emphasis role="italic">Colorless green ideas
                sleep furiously</emphasis> contains four bigrams:<itemizedlist>
                <listitem>
                    <para>Colorless, green</para>
                </listitem>
                <listitem>
                    <para>green, ideas</para>
                </listitem>
                <listitem>
                    <para>ideas, sleep</para>
                </listitem>
                <listitem>
                    <para>sleep, furiously</para>
                </listitem>
            </itemizedlist></para>
        <para>To stick to our ‘window’ analogy, we could say that all bigrams of a sentence can be
            found by placing a window on its first two words, and by moving this window to the right
            one word at a time in a stepwise manner. We then repeat this procedure, until the window
            covers the last two words of a sentence. In fact, the same holds for unigrams and
            N-grams with <emphasis role="italic">n</emphasis> greater than two. So, say we have a
            body of text represented as a list of words or tokens (whatever you prefer). For the
            sake of legacy, we will stick to a list of tokens representing the sentence <emphasis
                role="italic">Colorless green ideas sleep furiously</emphasis>:</para>
        <screen>Prelude> <userinput>["Colorless", "green", "ideas", "sleep", "furiously"]</userinput>
["Colorless","green","ideas","sleep","furiously"]</screen>
        <para>Hey! That looks like… indeed, that looks like a list of unigrams! Well, that was
            convenient. Unfortunately, things do not remain so simple if we move from unigrams to
            bigrams or <emphasis role="italic">some-very-large-n-grams</emphasis>. Bigrams and
            n-grams require us to construct 'windows' that cover more than one word at a time. In
            case of bigrams, for instance, this means that we would like to obtain a list of lists
            of two words (bigrams). Represented in such a way, the list of bigrams in the sentence
                <emphasis role="italic">Colorless green ideas sleep furiously</emphasis> would look
            like this:</para>
        <screen><userinput>[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</userinput></screen>
        <para>To arrive at such a list, we could start out with a list of words (yes indeed, the
            unigrams), and complete the following sequence of steps: <orderedlist>
                <listitem>
                    <para>Place a window on the first bigram, and add it to our bigram list</para>
                </listitem>
                <listitem>
                    <para>Move the window one word to the right</para>
                </listitem>
                <listitem>
                    <para>Repeat from the first step, until the last bigram is stored</para>
                </listitem>
            </orderedlist>Provided these steps, we first need a way to place a window on the first
            bigram, that is, we need to isolate the first two items of the list of words. In its
            prelude, Haskell defines a function named <emphasis role="italic">take</emphasis> that
            seems to suit our needs: </para>
        <screen>Prelude> <userinput>:type take</userinput> 
take :: Int -> [a] -> [a]</screen>
        <para>This function takes an <emphasis role="italic">Integer</emphasis> denoting <emphasis
                role="italic">n</emphasis> number of elements, and a list of some type <emphasis
                role="italic">a</emphasis>. Given these arguments, it returns the first <emphasis
                role="italic">n</emphasis> elements of a list of <emphasis role="italic"
                >a</emphasis>s. Thus, passing it the number two and a list of words should give
            us... our first bigram:</para>
        <screen>Prelude> <userinput>take 2 ["Colorless", "green", "ideas", "sleep", "furiously"]</userinput>
["Colorless","green"]</screen>
        <para>Great! That worked out nice! Now from here on, the idea is to add this bigram to a
            list, and to move the window one word to the right, so that we obtain the second bigram.
            Let us first turn to the latter (as we will get the list part for free later on). How do
            we move the window one word to the right? That is, how do we extract the second and
            third word in the list, instead of the first and second? A possible would be to use
            Haskell's <emphasis role="italic">!!</emphasis> operator: </para>
        <screen>Prelude> <userinput>:t (!!)</userinput>
(!!) :: [a] -> Int -> a</screen>
        <para>This operator takes a list of <emphasis role="italic">a</emphasis>s, and returns the
                <emphasis role="italic">n</emphasis>th element;</para>
        <screen>Prelude> <userinput>["Colorless", "green", "ideas", "sleep", "furiously"] !! 1</userinput> 
"green"
Prelude> <userinput>["Colorless", "green", "ideas", "sleep", "furiously"] !! 2</userinput> 
"ideas"</screen>
        <para>Great, this gives us the two words that make up the second bigram. Now all we have to
            do is stuff them together in a list: </para>
        <screen>Prelude> <userinput>["Colorless", "green", "ideas", "sleep", "furiously"] !! 1 : 
["Colorless", "green", "ideas", "sleep", "furiously"] !! 2 : []</userinput> 
["green","ideas"]</screen>
        <para>Well, this does the trick. However, it is not very convenient to wrap this up in a
            function, and moreover, this approach is not very Haskellish. In fact, there is a better
            and more elegant solution, namely to move the list instead of the window. Wait! What?
            Yes, move the list instead of the window. But how? Well, we could just look at the first
            and second word in the list again, after getting rid of the (previous) first word. In
            other words, we could look at the first two words of the tail of the list of words: </para>
        <screen>Prelude> <userinput>take 2 (tail ["Colorless", "green", "ideas", "sleep", "furiously"])</userinput>
["green","ideas"]</screen>
        <para>Now that looks Haskellish! What about the next bigram? and the one after that? Well,
            we could apply the same trick over and over again. We can look at the first two words of
            the tail of the tails of the list of words:</para>
        <screen>Prelude> <userinput>take 2 (tail (tail ["Colorless", "green", "ideas", "sleep", "furiously"]))</userinput>
["ideas","sleep"]</screen>
        <para>... and the tail of the tail of the tail of the list of words:</para>
        <screen>Prelude> <userinput>take 2 (tail (tail (tail ["Colorless", "green", "ideas", "sleep", "furiously"])))</userinput>
["sleep","furiously"]</screen>
        <para>In fact, that last step already gives us the last bigrams in the sentence <emphasis
                role="italic">Colorless green ideas sleep furiously</emphasis>. The last step would
            be to throw all these two word lists in a larger list, and we have ourselves a list of
            bigrams. However, whereas this is manageable by hand for this particular example, think
            about obtaining all the bigrams in the Brown corpus in this manner (gives you
            nightmares, doesn't it?). Indeed, we would rather like to wrap this approach up in a
            function that does all the hard word for us. Provided a list, this function should take
            its first two arguments, and then repetitively do this for the tail of this, and the
            tail of the tail of this list, and so forth. In other words, it should simply constantly
            take the first bigram of a list, and do the same for its
            tail:<programlisting>bigram :: [a] -> [[a]]
bigram xs = take 2 xs : bigram (tail xs)</programlisting></para>
        <para>Wow! That almost looks like black magic, doesn't it? The type signature reveals that
            the function <emphasis role="italic">bigram</emphasis> takes a list of <emphasis
                role="italic">a</emphasis>s, and returns a list of list of <emphasis role="italic"
                >a</emphasis>s. The latter could be a list of bigrams, so this looks promising. The
            function takes the first two elements of the list of <emphasis role="italic"
                >a</emphasis>s, and places them in front of the result of applying the same function
            to the tail of the list of <emphasis role="italic">a</emphasis>s. Eehh.. what?
            Congratulations! You have just seen your first share of <emphasis role="italic"
                >recursion</emphasis> magic (or madness). A recursive function is a function that
            calls itself, and whereas it might look dazzling on first sight, this function actually
            does nothing more than what we have done by hand in the above. It collects the first two
            elements of a list, and then does the same for the tail of this list. Moreover, it
            stuffs the two word lists in a larger list on the fly (we told you the list stuff would
            come in for free, didn't we?). But wait, will this work? Well, let us put it to a
            test:<screen>Prelude> <userinput>bigram ["Colorless", "green", "ideas", "sleep", "furiously"]</userinput>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"],
["furiously"],[],*** Exception: Prelude.tail: empty list</screen></para>
        <para>And the answer is... almost! The function gives us the four bigrams, but it seems to
            be too greedy: it does not stop looking for bigrams after collecting the last bigram in
            the list of words. But did we tell it when to stop then? Nope, we didn't. In fact, we
            have only specified a so-called <emphasis role="italic">recursive step</emphasis> of our
            recursive function. What we miss is what is called a <emphasis role="italic">stop
                condition</emphasis> (also known as a <emphasis role="italic">base case</emphasis>).
            In a recursive definition, a stop condition defines when a function should stop calling
            itself, that is, when our recursive problem is solved. In absence of a stop condition, a
            recursive function will keep calling itself for eternity. In fact, this explains above
            the error, we didn't specify a stop condition so the function will keep looking for
            bigrams for eternity. However, as the list of words is finite, the function will run
            into trouble when trying to look for bigrams in the tail of an empty list, and this is
            exactly what the exception tells us. So, how to fix it? Well, add a stop condition that
            specifies that we should stop looking for bigrams when the tail of a list contains only
            one item (as it is difficult to construct a bigram out of only one word). We could do
            this using an if..then..else structure:</para>
        <programlisting>bigram :: [a] -> [[a]]
bigram xs = if length(xs) >= 2
	then take 2 xs : bigram (tail xs)
	else []</programlisting>
        <para>This should solve our
            problems:<screen>Prelude> <userinput>bigram ["Colorless", "green", "ideas", "sleep", "furiously"]</userinput>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</screen></para>
        <para>And, indeed it does. When there is only one word left in our list of words, the bigram
            function returns an empty list, and moreover, it will stop calling itself therewith
            ending the recursion. So, lets see how this works with an artificial example. First we
            will recursively apply the bigram function until it is applied to a list that has less
            than two elements:</para>
        <screen>Prelude> <userinput>bigram [1,2,3,4]</userinput>
bigram [1,2,3,4] = [1,2] : bigram (tail [1,2,3,4])
bigram [2,3,4] = [2,3] : bigram (tail [2,3,4])
bigram [3,4] = [3,4] : bigram (tail [3,4])
bigram [4] = []</screen>
        <para>Application of the bigram function to a list with less than two elements results in an
            empty list. Moreover, the bigram function will not be applied recursively again as we
            have reached our stop condition. Now, the only thing that remains is to <emphasis
                role="italic">unwind</emphasis> the recursion. That is, we have called the bigram
            function from within itself for three times, and as we have just found the result to its
            third and last self call, we can now reversely construct the result of the outermost
            function call:</para>
        <screen>bigram [3,4] = [3,4] : []
bigram [2,3,4] = [2,3] : [3,4] : []
bigram [1,2,3,4] = [1,2] : [2,3] : [3,4] : []
[[1,2],[2,3],[3,4]]</screen>
        <para>Great! Are you still with us? As L. Peter Deutsch put it: "to iterate is human, to
            recurse divine." Whereas recursive definitions may seem difficult on first sight, you
            will find they are very powerful once you get the hang of them. In fact, they are very
            common in Haskell, and this will certainly be the first of many to come in the course of
            this book. Lets stick to the bigram function a little longer, because whereas the above
            works, it is aesthetically unpleasing. That is, we used an if..then..else structure to
            define our stop condition, but Haskell provides a more elegant way to do this through
            so-called <emphasis role="italic">pattern matching</emphasis>. Pattern matching can be
            thought of as defining multiple definition of the same functions, each tailored and
            honed for a specific argument pattern. Provided an argument, Haskell will then pick the
            first matching definition of a function, and return the result its application. Hence,
            we can define patterns for the stop condition and recursive step as follows:</para>
        <programlisting>bigram :: [a] -> [[a]]
bigram [x] = []
bigram xs  = take 2 xs : bigram (tail xs)</programlisting>
        <para>The second line represents the stop condition, and the third the familiar recursive
            step. Provided the list of words in the sentence <emphasis role="italic">Colorless green
                ideas sleep furiously</emphasis>, Haskell will match this to the recursive step, and
            apply this definition of the function to the list. When the recursive step calls the
            bigram function with a list that contains only one word (indeed, the tail of the list
            containing the last bigram), Haskell will match this call with the stop condition. The
            result of this call will simply an empty list. Lets first proof that this indeed
            works:<screen>Prelude> <userinput>bigram ["Colorless", "green", "ideas", "sleep", "furiously"]</userinput>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</screen>It
            did! To make the working of the use of pattern matching more insightful we can again
            write out an artificial example in
            steps:<screen>Prelude> <userinput>bigram [1,2,3,4]</userinput>
bigram [1,2,3,4] = [1,2] : bigram (tail [1,2,3,4])
bigram [2,3,4] = [2,3] : bigram (tail [2,3,4])
bigram [3,4] = [3,4] : bigram (tail [3,4])
bigram [4] = []
bigram [3,4] = [3,4] : []
bigram [2,3,4] = [2,3] : [3,4] : []
bigram [1,2,3,4] = [1,2] : [2,3] : [3,4] : []
[[1,2],[2,3],[3,4]]</screen>Check?
            We are almost there now. There two things left that we should look at before we mark our
            function as production ready. The first is a tiny aesthetically unpleasing detail. In
            the pattern of our step condition we use the variable <emphasis role="italic"
                >x</emphasis>, whereas we do not use this variable in the body of the function. It
            is therefore not necessary to bind the list element to this variable. Fortunately,
            Haskell provides a pattern that matches anything, without doing binding. This pattern is
            represented by an underscore. Using this underscore, we can patch up the aesthetics of
            our
            function:<programlisting>bigram :: [a] -> [[a]]
bigram [_] = []
bigram xs  = take 2 xs : bigram (tail xs)</programlisting></para>
        <para>Secondly, our function fails if we apply it to an empty
            list:<screen>Prelude> <userinput>bigram []</userinput>
[[]*** Exception: Prelude.tail: empty list</screen></para>
        <para>But hey! That error message looks familiar, doesn't it? Our function fails, again
            because we attempted to extract a bigram from the tail of an empty list. Indeed, an
            empty list does not match with the pattern of our stop condition, and therefore the
            recursive step is applied to it. We can solve this by adding a pattern for an empty
            list:</para>
        <programlisting>bigram :: [a] -> [[a]]
bigram []  = []
bigram [_] = []
bigram xs  = take 2 xs : bigram (tail xs)</programlisting>
        <para>This new pattern basically states that the list of a bigrams of an empty word list is
            in turn an empty list. This assures that our function will not fail when applied to an
            empty list:</para>
        <para>
            <screen>Prelude> <userinput>bigram []</userinput>
[]</screen>
        </para>
        <para>If you want to get really fancy, you could also use pattern matching to extract a
            bigram, rather than using
            <function>take</function>:<programlisting>bigram' :: [a] -> [[a]]
bigram' (x:y:xs) = [x,y] : bigram' (y:xs)
bigram' _        = []</programlisting></para>
        <para>Now, we only need to account for two patterns: the first pattern matches when the list
            has at least two elements. The second pattern matches the empty list and the list
            containing just one element.</para>
        <para>Good, we are all set! We have our bigram function now... time for some applications of
            a bigram language model!</para>
        <sect2>
            <title xml:id="chap-ngrams-bigrams-exercises">Exercises</title>
            <orderedlist>
                <listitem>
                    <para>A skip-bigram is any pair of words in sentence order. Write a function
                            <function>skipBigrams</function> that extracts skip-bigrams from a
                        sentence as a list of binary tuples, using explicit recursion. Running your
                        function on <emphasis>["Colorless", "green", "ideas", "sleep",
                            "furiously"]</emphasis> should give the following
                        output:<screen>Prelude> skipBigrams ["Colorless", "green", "ideas", "sleep", "furiously"]
[("Colorless","green"),("Colorless","ideas"),("Colorless","sleep"),
("Colorless","furiously"),("green","ideas"),("green","sleep"),
("green","furiously"),("ideas","sleep"),("ideas","furiously"),
("sleep","furiously")]</screen>
                    </para>
                </listitem>
            </orderedlist>
        </sect2>
    </sect1>
    <sect1 xml:id="chap-ngrams-pattern-matching">
        <title>A few words on Pattern Matching</title>
        <para>Stub</para>
    </sect1>
    <sect1 xml:id="chap-ngrams-collocations">
        <title>Collocations</title>
        <para>A straightforward application of bigrams is the identification of so-called <emphasis
                role="italic">collocations</emphasis>. Recall that bigram language models exploit
            the observations that words do not simply combine in any random order, that is, word
            order is constraint by grammatical structure. However, some combinations of words are
            subject to an additional law of constraint. This law enforces a combination of two words
            to occur relatively more often together than in absence of each other. Such combinations
            are commonly known as collocations. Depending on the corpus, examples of collocations are:<itemizedlist>
                <listitem>
                    <para>United States</para>
                </listitem>
                <listitem>
                    <para>vice president</para>
                </listitem>
                <listitem>
                    <para>chief executive</para>
                </listitem>
            </itemizedlist></para>
        <para>Corpus linguists study such collocations to answer interesting questions about the
            combinatory properties of words. An example of such a question concerns the combination
            of verbs and prepositions: does the verb to govern occur more often in combination with
            the preposition <emphasis role="italic">by</emphasis> than with the preposition
                <emphasis role="italic">with</emphasis>?.</para>
        <para>In the present section, we will investigate collocations in the Brown corpus. But
            before we do so, we first turn to the question of how to identify collocations. A simple
            but effective approach to collocation identification is to compare the <emphasis
                role="italic">observed</emphasis> chance of observing a combination of two words to
            the <emphasis role="italic">expected</emphasis> chance. How does this work? Well, say we
            have a 1000 word corpus in which the word <emphasis role="italic">vice</emphasis> occurs
            50 times, and the word <emphasis role="italic">president</emphasis> 100 times. In other
            words, the chance that a randomly picked word is the word <emphasis role="italic"
                >vice</emphasis> is <emphasis role="italic">p(vice)</emphasis> = 50/1000 = 0.05. In
            similar fashion, the chance that randomly picked word is the word president is <emphasis
                role="italic">p(president)</emphasis> = 100/1000 = 0.1. Now what would be the chance
            of observing the combination <emphasis role="italic">vice president</emphasis> if the
            word <emphasis role="italic">vice</emphasis> and <emphasis role="italic"
                >president</emphasis> were "unrelated"? Well, this would simply be the chance of
            observing the word <emphasis role="italic">vice</emphasis> multiplied by the chance of
            observing the word <emphasis role="italic">president</emphasis>. Thus, <emphasis
                role="italic">p(vice president)</emphasis> = 0.05 x 0.01 = 0.005. From our thousand
            word corpus, we can extract 1000 - 1 = 999 bigrams. Assume that the bigram <emphasis
                role="italic">vice president</emphasis> occurs 40 times, meaning that the chance of
            observing this combination in our corpus is <emphasis role="italic">p(vice
                president)</emphasis> = 40 / 999 = 0.04. This reveals the observed chance of
            observing the combination <emphasis role="italic">vice president</emphasis> is larger
            than the expected chance. In fact we can quantify this difference in observed and
            expected chance for any two words <emphasis role="italic">W1</emphasis> and <emphasis
                role="italic">W2</emphasis>:</para>
        <para>
            <equation>
                <title>Difference between observed and expected chance</title>
                <xi:include href="eq-obex-diff.mml"/>
            </equation>
        </para>
        <para>The observed chance of observing the combination <emphasis role="italic">vice
                president</emphasis> is eight times larger than the expected chance of observing
            this combination. The difference between the observed and expected chance will be large
            for words that occur together a lot of times, whereas it will be small for words that
            also occur relatively often independent of each other. </para>
        <para>Provided this measure of difference between the observed and expected chance, we can
            identify the strongest collocations in a corpus by means of three steps:</para>
        <para>
            <orderedlist>
                <listitem>
                    <para>Extract all the bigrams from the corpus</para>
                </listitem>
                <listitem>
                    <para>Compute the difference between the observed and expected chance for each
                        bigram</para>
                </listitem>
                <listitem>
                    <para>Rank the bigrams based on these differences</para>
                </listitem>
            </orderedlist>
        </para>
        <para>The bigrams with the highest difference between observed and expected chance reflect
            the strongest collocations. However, the difference between observed and expected
            chances might easily become very large. To condense these difference values, we can
            represent them in logarithmic space. By doing so, we have stumbled upon a very frequent
            used measure of association: the so-called Pointwise Mutual Information (PMI). The PMI
            value for the combination of the <emphasis role="italic">vice president</emphasis> is: </para>
        <para>
            <equation>
                <title>Pointwise mutual information</title>
                <xi:include href="eq-pmi.mml"/>
            </equation>
        </para>
        <para>Provided this association measure, we can replace step two in three steps above with:
            compute the PMI between the obseved and expected chance for each bigram.</para>
        <para>Now that we know how to identify collocations, we can apply our knowledge to the Brown
            corpus. First we have to read in the contents of this corpus like we learned in the
            previous chapter:</para>
        <para>
            <screen>*Main> <userinput>h &lt;- IO.openFile "brown.txt" IO.ReadMode</userinput>
*Main> <userinput>c &lt;- IO.hGetContents h</userinput></screen>
        </para>
        <para>Good! From here on, let us first obtain a list of bigrams for this corpus:</para>
        <screen>*Main> <userinput>let bgs = bigrams (words c)</userinput>
*Main> <userinput>head bgs</userinput>
["The","Fulton"]</screen>
        <para>As a sanity check, we could verify whether we indeed obtained all the bigrams in the
            corpus. For a corpus of <emphasis role="italic">n</emphasis> words, we expect <emphasis
                role="italic">n-1</emphasis>
            bigrams:<screen>*Main> <userinput>length (words c)</userinput>
1165170
*Main> <userinput>length bgs</userinput>
1165169</screen></para>
        <para>That looks great! Next we need to determine the relative frequency of each of these
            bigrams in the corpus. That is, for each bigram we need to determine the observed chance
            of observing it. We could start by determining the frequency of each bigram. We can
            reuse the <emphasis role="italic">freqList</emphasis> function defined in the previous
            chapter to
            so:<screen>*Main> <userinput>Data.Map.lookup ["United","States"] (freqList bgs)</userinput>
Just 392</screen></para>
        <para>Todo: finish this section</para>
    </sect1>
    <sect1 xml:id="chap-ngrams-ngrams">
        <title>From bigrams to n-grams</title>
        <para>While extracting collocations from the Brown corpus, we have seen how useful bigrams
            actually are. But at this point you may be clamoring for the extraction of collocations
            of three or more words. For this and many other tasks, it is useful to extract so-called
                <emphasis role="italic">n-grams</emphasis> for an arbitrary <emphasis role="italic"
                >n</emphasis>. We can easily modify our definition of bigrams to extract n-grams a
            specified length. Rather than always <function>take</function>ing two elements, we make
            the number of items to take an argument to the function: </para>
        <para>
            <programlisting>ngrams :: Int -> [a] -> [[a]]
ngrams 0 _  = []
ngrams _ [] = []
ngrams n xs
  | length ngram == n = ngram : ngrams n (tail xs)
  | otherwise         = []
  where
    ngram = take n xs</programlisting>
        </para>
        <para>We also cannot use pattern matching to exclude the tail when it is shorter than
                <emphasis>n</emphasis>. Instead, we add a guard that ends the recursion if we cannot
            get the proper number of elements from the list. This function works as you would
            expect:<screen>Prelude> <userinput>ngrams 3 [1..10]</userinput>
[[1,2,3],[2,3,4],[3,4,5],[4,5,6],
 [5,6,7],[6,7,8],[7,8,9],[8,9,10]]
Prelude> <userinput>ngrams 8 [1..10]</userinput>
[[1,2,3,4,5,6,7,8],[2,3,4,5,6,7,8,9],
 [3,4,5,6,7,8,9,10]]</screen></para>
        <para>Since this is barely worth a section, we will take this opportunity to show two other
            implementations of the <function>ngrams</function> function. The first will be more
            declarative than the definition above, the second will make use of a monad that we have
            not used yet: the list monad.</para>
        <sect2>
            <title>A declarative definition of ngrams</title>
            <para>Some patterns emerge in the recursive definition of <function>ngrams</function>
                that correspond to functions in the <emphasis>Data.List</emphasis> module:<orderedlist>
                    <listitem>
                        <para>Every recursive call uses the tail of the list. In other words, we
                            enumerate every tail of the list, including the complete list. The
                                <function>Data.List.tails</function> function provides exactly this
                            functionality.</para>
                    </listitem>
                    <listitem>
                        <para>We extract the first <emphasis>n</emphasis> elements from every tail.
                            This is a mapping over the data that could be performed with the
                                <function>map</function> function.</para>
                    </listitem>
                    <listitem>
                        <para>The guards in the recursive case amount to filtering lists that do not
                            have length <emphasis>n</emphasis>. Such filtering can also be performed
                            by the <function>filter</function> function.</para>
                    </listitem>
                </orderedlist></para>
            <para>Let's go through each of these patterns to compose a declarative definition of
                    <function>ngrams</function>. First, we extract the tails from the list, using
                the <function>tails</function>
                function:<screen>Prelude> <userinput>import Data.List</userinput>
Prelude Data.List> <userinput>let sent = ["Colorless", "green", "ideas", "sleep", "furiously"]</userinput>
Prelude Data.List> <userinput>tails sent</userinput>
[["Colorless","green","ideas","sleep","furiously"],
["green","ideas","sleep","furiously"],["ideas","sleep","furiously"],
["sleep","furiously"],["furiously"],[]]</screen></para>
            <para>This gives us a list of tails, including the complete sentence. Now, we
                    <function>map</function>
                <function>take</function> over each tail to extract an n-gram. Since
                    <function>take</function> requires two arguments, we use currying to bind the
                first argument. For now. we will use <function>take 2</function> to extract
                bigrams:<screen>Prelude Data.List> <userinput>map (take 2) $ tails sent</userinput>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"],["furiously"],[]]</screen></para>
            <para>This comes close to a list of bigrams, except that we have an empty list and a
                list with just one member dangling at the end. These anomalies are perfect
                candidates to be filtered out, so we use the <function>filter</function> function in
                conjunction with the <function>length</function> function to exclude any element
                that is not of the given length. To accomplish this, we apply some currying
                awesomeness. Remember that we can convert infix operators to prefix operators by
                adding
                parentheses:<screen>Prelude Data.List> <userinput>(==) 2 2</userinput>
True
Prelude Data.List> <userinput>(==) 2 3</userinput>
False</screen></para>
            <para>This shows that <function>==</function> is just an ordinary function, that just
                happens to use the infix notation for convenience. Since this is an ordinary
                function, we can also apply
                currying:<screen>Prelude Data.List> <userinput>let isTwo = (==) 2</userinput>
Prelude Data.List> <userinput>isTwo 2</userinput>
True
Prelude Data.List> <userinput>isTwo 3</userinput>
False</screen></para>
            <para>Ok, so we want to check whether a list has two elements, so we could just apply
                    <function>isTwo</function> to the result of the <function>length</function>
                function:<screen>Prelude Data.List> <userinput>isTwo (length ["Colorless","green"])</userinput>
True
Prelude Data.List> <userinput>isTwo (length [])</userinput>
False</screen></para>
            <para>Or, written as a function
                definition:<programlisting>hasLengthTwo l = isTwo (length l)</programlisting></para>
            <para>Since this function follows the canonical form <emphasis>f (g x)</emphasis>, we
                can use function
                composition:<screen>Prelude Data.List> <userinput>let hasLengthTwo = isTwo . length</userinput>
Prelude Data.List> <userinput>hasLengthTwo ["Colorless","green"]</userinput>
True</screen></para>
            <para>Our filtering expression, <emphasis>(==) 2 . length</emphasis>, turns out to be
                quite compact. Time to test this with our not-yet-correct list of
                bigrams:<screen>Prelude Data.List> <userinput>filter ((==) 2 . length) $ map (take 2) $ tails sent</userinput>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</screen></para>
            <para>And this corresponds to the output we expected. So, we can now wrap this
                expression in a function, replacing <emphasis>2</emphasis> by
                <emphasis>n</emphasis>:<programlisting>ngrams' :: Int -> [b] -> [[b]]
ngrams' n = filter ((==) n . length) . map (take n) . tails</programlisting></para>
            <para>This function is equivalent to <function>ngrams</function> for all given
                lists.</para>
            <para>You may wonder why this exercise is worthwhile. The reason is that the
                declarativeness of <function>ngrams'</function> makes the function much easier to
                read. We can almost immediately see what this function does by reading its body
                right-to-left, while the recursive definition requires a closer look. You will
                notice that, as you get more familiar with Haskell, it will become easier to spot
                such patterns in functions.</para>
        </sect2>
        <sect2>
            <title>A monadic definition of ngrams</title>
            <para>As discussed in the previous chapter, each type that belongs to the
                    <classname>Monad</classname> typeclass provides the <function>(>>=)</function>
                function to combine expressions resulting in that type. The list type also belongs
                to the monad type class. In GHCi, you can use the <command>:info</command> command
                to list the type classes to which a type
                belongs:<screen>Prelude> <userinput>:i []</userinput>
data [] a = [] | a : [a] 	-- Defined in GHC.Types
instance Eq a => Eq [a] -- Defined in GHC.Classes
instance Monad [] -- Defined in GHC.Base
instance Functor [] -- Defined in GHC.Base
instance Ord a => Ord [a] -- Defined in GHC.Classes
instance Read a => Read [a] -- Defined in GHC.Read
instance Show a => Show [a] -- Defined in GHC.Show</screen></para>
            <para>The third line of the output shows that lists belong to the
                    <classname>Monad</classname> type class. But how does the
                    <function>(>>=)</function> function combine expressions resulting in a list? A
                quick peek at its definition for the list type reveals
                this:<programlisting>instance Monad [] where
  m >>= k = foldr ((++) . k) [] m
  [...]</programlisting></para>
            <para>So, the join operation takes a list <emphasis>m</emphasis>, applies a function
                    <function>k</function> to each element and concatenates the results. Of course,
                this concatenation implies that <function>k</function> itself should evaluate to a
                list, making the type signature of <emphasis>k</emphasis> as follows: <function>k ::
                    a -> [a]</function></para>
            <para>We will illustrate this with an example. Suppose that we would want to calculate
                the immediate predecessor and successor of every number in the list
                    <emphasis>[0..9]</emphasis>. In this case, we could use the function
                    <function>\x -> [x-1,x+1]</function> in the list
                monad:<screen>Prelude> <userinput>:{
do
  l  &lt;- [0..9]
  ps &lt;- (\x -> [x-1,x+2]) l
  return ps
:}</userinput>
[-1,2,0,3,1,4,2,5,3,6,4,7,5,8,6,9,7,10,8,11]
</screen></para>
            <para>First, the list is bound to <emphasis>l</emphasis>, then our predecessor/successor
                function is applied to <emphasis>l</emphasis>. Since we are using this function in
                the context of the list monad, the function is be applied to every member of
                    <emphasis>l</emphasis>. The results of these applications is
                concatenated.</para>
            <note>
                <para>Experimenting with list monads may give you results that may be surprising at
                    first sight. For
                    instance:<screen>Prelude> <userinput>:{
do
  l &lt;- [0..9]
  m &lt;- [42,11]
  return m
:}</userinput>
[42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11]</screen></para>
                <para>Since <emphasis>[42,11]</emphasis> in <emphasis>m &lt;- [42,11]</emphasis>
                    does not use an argument, its corresponding function is <function>\_ ->
                        [42,11]</function>. Since <function>foldr</function> still traverses the
                    list bound to <emphasis>l</emphasis>, the monadic computation is equal
                    to:<screen>Prelude> <userinput>foldr ((++) . (\_ -> [42,11])) [] [0..9]</userinput>
[42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11]</screen></para>
            </note>
            <para>We can also extract bigrams using the list monad. Given a list of tails, we could
                extract the first two words of each tail using
                <function>take</function>:<screen>Prelude> <userinput>import Data.List</userinput>
Prelude Data.List> <userinput>let sent = ["Colorless", "green", "ideas", "sleep", "furiously"]</userinput>
Prelude Data.List> <userinput>:{
do
  t &lt;- tails sent
  l &lt;- take 2 t
  return l
:}</userinput>
["Colorless","green","green","ideas","ideas","sleep","sleep","furiously","furiously"]</screen></para>
            <para>That's close. However, since the list monad concatenates the results of every
                    <emphasis>take 2 t</emphasis> expression, we cannot directly identify the
                n-grams anymore. This is easily remedied by wrapping the result of
                    <function>take</function> in a
                list:<screen>Prelude Data.List><userinput> :{
do
  t &lt;- tails sent
  l &lt;- [take 2 t]
  return l
:}</userinput>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"],["furiously"],[]]</screen></para>
            <para>Now we get the n-grams nicely as a list. However, as in previous definitions of
                    <function>ngrams</function> we have to exclude lists that do not have the
                requested number of elements. We could, as we did previously, filter out these
                members using
                <function>filter</function>:<screen>Prelude Data.List> <userinput>:{
filter ((==) 2 . length) $ do
  t &lt;- tails sent
  l &lt;- [take 2 t]
  return l
:}</userinput>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</screen></para>
            <para>But that would not be a very monadic way to perform this task. It would be nice if
                we could just choose elements to our liking. Such a (monadic) choice function
                exists, namely
                <function>Control.Monad.guard</function>:<screen>Prelude Data.List> <userinput>import Control.Monad</userinput>
Prelude Data.List Control.Monad> <userinput>:type guard</userinput>
guard :: MonadPlus m => Bool -> m ()</screen></para>
            <para><function>guard</function> is a function that takes a boolean, and returns
                something that is a <classname>MonadPlus</classname>. Whoa! For now, accept that the
                list type belongs to the <classname>MonadPlus</classname> type class (after
                importing <emphasis>Control.Monad</emphasis>). Instead of going into the working of
                    <classname>MonadPlus</classname> now, we will perform a behavioral study of
                    <function>guard</function>:<screen>Prelude Data.List Control.Monad> <userinput>:{
do
  l &lt;- [0..9]
  guard (even l)
  return l
:}</userinput>
[0,2,4,6,8]</screen></para>
            <para>Funky huh? We used <function>guard</function> to enumerate just those numbers from
                    <emphasis>[0..9]</emphasis> that are even. Of course, we could as well use
                    <function>guard</function> in our bigram extraction to filter lists that are not
                of a certain
                length:<screen>Prelude Data.List Control.Monad> <userinput>:{
do
  t &lt;- tails sent
  l &lt;- [take 2 t]
  guard (length l == 2)
  return l
:}</userinput>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</screen></para>
            <para>Ain't that beautiful? We applied a guard to pick just those elements that are of
                length <emphasis>2</emphasis>, or as you might as well say, we put a constraint on
                the list requiring elements to be of length <emphasis>2</emphasis>. We can easily
                transform this expression to a function, by making the n-gram length and the list
                arguments of that
                function:<programlisting>ngrams'' :: Int -> [a] -> [[a]]
ngrams'' n l = do
  t &lt;- tails l
  l &lt;- [take n t]
  guard (length l == n)
  return l </programlisting></para>
            <para>As you can conclude from the previous sections, there is often more than one way
                to implement a function. In practice you will want to pick a declaration that is
                readable and performant. In this case, we think that the declarative definition of
                    <function>ngrams</function> is the most preferable.</para>
        </sect2>
        <sect2>
            <title>Exercises</title>
            <orderedlist>
                <listitem>
                    <para>Rewrite the <function>skipBigram</function> function discussed in <xref
                            linkend="chap-ngrams-bigrams-exercises"/> without explicit recursion,
                        either by defining it more declaratively or using the list monad. Hint: make
                        use of the <function>Data.List.zip</function> function.</para>
                </listitem>
            </orderedlist>
        </sect2>
    </sect1>
    <sect1 xml:id="chap-ngrams-lazy-strict">
        <title>Lazy and strict evaluation</title>
        <para>You may have noticed that something curious goes on in Haskell. For instance, consider
            the following GHCi
            session:<screen>Prelude> <userinput>take 10 $ [0..]</userinput>
[0,1,2,3,4,5,6,7,8,9]</screen></para>
        <para>The expression <function>[0..</function> is the list of numbers from zero to infinity.
            Obviously, it is impossible to store an infinite list in finite memory. Haskell does not
            apply some simple trick, since it also works in less trivial cases. For
            instance:<screen>Prelude> <userinput>take 10 $ filter even [0..]</userinput>
[0,2,4,6,8,10,12,14,16,18]</screen></para>
        <para>This also works for your own
            predicates:<screen>Prelude> <userinput>let infinite n = n : infinite (n + 1)</userinput>
Prelude> <userinput>take 3 $ infinite 0</userinput>
[0,1,2,3]</screen></para>
        <para>In most other programming languages, this computation will never terminate, since it
            will go into an infinite recursion. Haskell, however, won't. The reason is that Haskell
            uses <emphasis role="italic">lazy evaluation</emphasis> - an expression is only
            evaluated when necessary. For instance, taking three elements from
                <function>infinite</function> results in the following
            evaluations:<programlisting>infinite 0
0 : infinite 1
0 : (1 : infinite 2)
0 : (1 : (2 : infinite 3))
0 : (1 : (2 : (3 : infinite 4)))</programlisting></para>
        <para>Once <function>take</function> has consumed enough elements from
                <function>infinite</function>, the tail of the list is the expression
                <function>infinite 4</function>. Since <function>take</function> does not need more
            elements, the tail is never evaluated. Lazy evaluation allows you to do clever tricks,
            such as defining infinite lists. The downside is that it is often hard to predict when
            an expression is evaluated, and what effect that has on performance of a program.</para>
        <para><emphasis role="italic">Todo: lazy evaluation and folds.</emphasis></para>
    </sect1>
    <sect1 xml:id="chap-ngrams-suffixarrays">
        <title>Suffix arrays</title>
        <para>In this chapter, we have seen how you could extract an n-gram of a given <emphasis
                role="italic">n</emphasis> from a list of words, characters, groceries, or whatever
            you desire. You can also store n-gram frequencies in a <type>Map</type>, to build
            applications that quickly need the frequency (or probability) of an n-gram in a corpus.
            What if you would encounter an application where you need access to n-grams of any
            length? Any! From unigrams to 'almost the length of your corpus'-grams. Obviously, if
            your corpus contains <emphasis role="italic">m</emphasis> elements, storing frequencies
            of all 1..m-grams would make your program a memory hog.</para>
        <para>Fortunately, it turns out that there is a simple and smart trick to do this, using a
            data structure called <emphasis role="italic">suffix arrays</emphasis>. First, we start
            with the corpus, and a parallel list or array where each element contains an index that
            can be seen as a pointer into the corpus. The left side of figure <xref
                linkend="fig-suffixarray"/> shows the initial state for the phrase "to be or not to
            be". We then sort the array of indices by comparing the elements they point to. For
            instance, we could compare the element with index 2 ("or") and the element with index 3
            ("not"). Since "not" is lexicographically ordered before "or", the list of indices
            should be sorted such that the element holding index 3 comes before 2. When two indices
            point to equal elements, e.g. 0 and 4 ("to"), we move on to the element that succeed
            both instances of "to", respectively "be" and "be". And we continue such comparisons
            recursively, until we find out that one n-gram is lexicographically sorted before the
            other (in this case, 4 should come before 0, since "to be" is lexicographically sorted
            before "to be or". The right side of figure <xref linkend="fig-suffixarray"/> shows how
            the indices will be sorted after applying this sorting methodology.</para>
        <figure xml:id="fig-suffixarray">
            <title>Constructing a suffix array</title>
            <informaltable frame="none" colsep="0">
                <tgroup cols="2">
                    <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                    <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                    <tbody>
                        <row>
                            <entry>
                                <mediaobject xml:id="fig-suffixarray-unsorted">
                                    <imageobject>
                                        <imagedata fileref="../images/suffixarray-unsorted.svg"
                                            format="svg" width="15em"/>
                                    </imageobject>
                                    <caption>
                                        <para>Unsorted indices</para>
                                    </caption>
                                </mediaobject>
                            </entry>
                            <entry>
                                <mediaobject>
                                    <imageobject>
                                        <imagedata fileref="../images/suffixarray-sorted.svg"
                                            width="15em"/>
                                    </imageobject>
                                    <caption>
                                        <para>Sorted indices</para>
                                    </caption>
                                </mediaobject>
                            </entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
        </figure>
        <para>After sorting the list of indices in this manner, the index list represents an ordered
            list of n-grams within the corpus. The length of the n-gram does not matter, since
            elements and their suffixes were compared until one element could be sorted
            lexicographically before the other. This ordering also implies that we can use a binary
            search to check whether an n-gram occurred in the corpus, and if so, how often. But more
            on that later...</para>
        <para>Of course, as a working programmer you can't wait to fire up your text editor to
            implement suffix arrays. It turns out to be simpler than you might expect. But, we need
            to introduce another data type first, the vector. It is a data type that is comparable
            to arrays in other programming languages. Vectors allow for random access to array
            elements. So, if you want to access the <emphasis role="italic">n</emphasis>-th element
            of a vector, it can be accessed directly, rather than first traversing the <emphasis
                role="italic">n-1</emphasis> preceding elements as in a list. Vectors are provided
            in Haskell as a part of the <package>vector</package> package that can be installed
            using <command>cabal</command>. We can construct a <type>Vector</type> from a list and
            convert a <type>Vector</type> to a
            list:<screen>Prelude> <userinput>Data.Vector.fromList ["to","or","not","to","be"]</userinput>
fromList ["to","or","not","to","be"] :: Data.Vector.Vector
Prelude> <userinput>Data.Vector.toList $ Data.Vector.fromList ["to","or","not","to","be"]</userinput>
["to","or","not","to","be"]</screen></para>
        <para>The <function>(!)</function> function is used to access an
            element:<screen>Prelude> <userinput>(Data.Vector.fromList ["to","or","not","to","be"]) Data.Vector.! 3</userinput>
"to"</screen></para>
        <para>There's also a safe access function, <function>(!?)</function>, that wraps the element
            in a <type>Maybe</type>. <type>Nothing</type> is returned when you use an index that is
            'outside' the
            vector:<screen>Prelude> <userinput>(Data.Vector.fromList ["to","or","not","to","be"]) Data.Vector.! 20</userinput>
"*** Exception: ./Data/Vector/Generic.hs:222 ((!)): index out of bounds (20,5)
Prelude> <userinput>(Data.Vector.fromList ["to","or","not","to","be"]) Data.Vector.!? 20</userinput>
Nothing
Prelude> <userinput>(Data.Vector.fromList ["to","or","not","to","be"]) Data.Vector.!? 3</userinput>
Just "to"</screen></para>
        <para>That enough for now. The primary reason why <type>Vector</type> is a useful type here,
            is because we want random access to the corpus during the construction of the suffix
            array. After construction, it is also useful for most tasks to be able to access the
            indices randomly. Alright, first we create a data type for the suffix
            array:<programlisting>import qualified Data.Vector as V

data SuffixArray a = SuffixArray (V.Vector a) (V.Vector Int)
                     deriving Show</programlisting></para>
        <para>It says exactly what we saw in the figure above: a suffix array consists of a data
            vector (in our case a corpus) and a vector of indices, respectively <type>V.Vector
                a</type> and <type>V.Vector Int</type>. Ideally, we would like to construct a suffix
            array from a list. However, to do this, we need a sorting function. The <emphasis
                role="italic">Data.List</emphasis> module contains the <function>sortBy</function>
            function that sorts a list according to some ordering
            function:<screen>*Main> <userinput>:type Data.List.sortBy</userinput>
Data.List.sortBy :: (a -> a -> Ordering) -> [a] -> [a]</screen></para>
        <para>So, it takes a comparison function that should compare two elements, and that returns
                <type>Ordering</type>. <type>Ordering</type> is a data type that specifies... order.
            There are three constructors: <type>LT</type>, <type>EQ</type>, and<type>GT</type>,
            these constructors indicate respectively that the first argument is less than, equal to,
            or greater than the second argument.</para>
        <para>We will use <function>sortBy</function> to sort the list of indices. Since the
            ordering of the indices is determined by elements of the data array, to which the
            indices refer, the comparison function that we provide for sorting the index array
            requires access to the data array. So, our function will compare (sub)vectors, indicated
            by their indices. This will work, since the <type>Data.Vector</type> data type is of the
                <type>Ord</type> type class, meaning that the operators necessary for comparisons
            are provided. Our comparison function can be written like
            this:<programlisting>saCompare :: Ord a => (V.Vector a -> V.Vector a -> Ordering) ->
             V.Vector a -> Int -> Int -> Ordering
saCompare cmp d a b = cmp (V.drop a d) (V.drop b d)</programlisting></para>
        <para>To allow a user of our function to impose their own sorting order (maybe the want to
            make a reversibly offered suffix array), we <function>saCompare</function> requires a
            comparison function as its first argument. The second argument is the data vector, and
            the final two arguments are the indices to be compared. We can get the subvectors
            represented by the two indices by using the <function>Data.Vector.drop</function>
            function. Suppose, if we want the element at index two, we can just drop the first two
            arguments, since we start counting at zero. We then use the provided comparison function
            to compare the two subvectors.</para>
        <para>Now we can create the function that actually creates a suffix
            array:<programlisting>import qualified Data.List as L

suffixArrayBy :: Ord a => (V.Vector a -> V.Vector a -> Ordering) ->
                 V.Vector a -> SuffixArray a
suffixArrayBy cmp d = SuffixArray d (V.fromList srtIndex)
    where uppBound = V.length d - 1
          usrtIndex = [0..uppBound]
          srtIndex = L.sortBy (saCompare cmp d) usrtIndex</programlisting></para>
        <para>This function is fairly simple, first we create the unsorted list of indices and bind
            it to <varname>usrtIndex</varname>. We construct this list by using a range. A range
            contains the indicated lower bound and upper bound, and all integers in
            between;<screen>*Main> <userinput>[0..9]</userinput>
[0,1,2,3,4,5,6,7,8,9,10]</screen></para>
        <para>We retrieve the upper bound using the <function>Data.Vector.length</function> function
            by subtracting one, since we are counting from zero. We then obtain the sorted index
                (<varname>srtIndex</varname>) by using the <function>Data.List.sortBy</function>
            function. This function takes a comparison function as its first argument and a list as
            its second
            argument:<screen>*Main> <userinput>:type Data.List.sortBy</userinput>
Data.List.sortBy :: (a -> a -> Ordering) -> [a] -> [a]</screen></para>
        <para>We can just plug in our <function>saCompare</function> function, which we pass a
            comparison function, and the data vector. Finally, we use the <type>SuffixArray</type>
            constructor to construct a <type>SuffixArray</type>, converting the list of indices to a
            vector. For convenience, we can also add a function that uses Haskell's
                <function>compare</function> function that uses the default sorting order that is
            imposed by the <type>Ord</type> typeclass:</para>
        <programlisting>suffixArray :: Ord a => V.Vector a -> SuffixArray a
suffixArray = suffixArrayBy compare</programlisting>
        <para>Neat! But as you have noticed by now, every serious data type has
                <function>fromList</function> and <function>toList</function> functions, so ours
            should have those as well. <function>fromList</function> is really simple; we can
            already construct a suffix array from a <type>Vector</type> using the
                <function>suffixArray</function> function. So, we just need to convert a list to a
                <type>Vector</type>, and pass it to
            <function>suffixArray</function>:<programlisting>fromList :: Ord a => [a] -> SuffixArray a
fromList = suffixArray . V.fromList</programlisting></para>
        <para>Easy huh? The <function>toList</function> is a bit more involved. First we have to
            decide what it should actually return. Providing the data vector as a list is not very
            useful, it's probably what someone started with. Returning a list of indices is more
            useful, but then we shift the burden off retrieving the n-grams that every index
            represents to the user of our suffix array. The most useful thing would be to return a
            list of all n-grams (of any length). So, for the phrase "to be or not to be", we want to
            return the following elements:<itemizedlist>
                <listitem>
                    <para>["be"]</para>
                </listitem>
                <listitem>
                    <para>["be","or","not","to","be"]</para>
                </listitem>
                <listitem>
                    <para>["not","to","be"]</para>
                </listitem>
                <listitem>
                    <para>["or","not","to","be"]</para>
                </listitem>
                <listitem>
                    <para>["to","be"]</para>
                </listitem>
                <listitem>
                    <para>["to","be","or","not","to","be"]</para>
                </listitem>
            </itemizedlist></para>
        <para>To achieve this, we need to extract the subvector for each index, in the order that
            the sorted vector of indices indicates. We can then convert each subvector to a list. We
            can use <function>Data.Vector.foldr</function> function to traverse the vector,
            constructing a list for each index. We will accumulate these lists in (yet another)
            list. Please welcome
            <function>toList</function>:<programlisting>toList :: SuffixArray a -> [[a]]
toList (SuffixArray d i) = V.foldr vecAt [] i
    where vecAt idx l = V.toList (V.drop idx d) : l</programlisting></para>
        <para>The <function>vecAt</function> function extracts a subvector starting at index
                <varname>idx</varname>, converts it to a list. We form a new list, with the
            accumulator as the tail, and the newly constructed 'subvector list' as the head. We use
                <function>foldr</function> to ensure that the list that is being constructed is in
            the correct order - since the accumulator becomes the tail, a <function>foldl</function>
            would make the first subarray the last in the list. Time to play with our new data type
            a
            bit:<screen>*Main> <userinput>toList $ fromList ["to","be","or","not","to","be"]</userinput>
[["be"],
["be","or","not","to","be"],
["not","to","be"],
["or","not","to","be"],
["to","be"],
["to","be","or","not","to","be"]]</screen></para>
        <para>Excellent, just as we want it: we get an ordered list of all n-grams in the corpus,
            for the maximum possible <emphasis role="italic">n</emphasis>. We can use this function
            to extract all
            bigrams:<screen>*Main> <userinput>filter ((== 2) . length) $ map (take 2) $ toList $ \
  fromList ["to","be","or","not","to","be"]</userinput></screen></para>
        <para>We extract the first two elements of each n-gram. This also gives us one unigram (the
            last token of the corpus), so we also have to filter the list for lists that contain two
            elements.</para>
        <para>After some celebrations and a cup of tea, it is time to use suffix arrays to find the
            frequency of a word. To do this, we use a binary search. For quick accessibility, we
            create a function comparable to the <function>toList</function> method, but returning a
                <type>Vector</type> of <type>Vector</type>, rather than a list of
            list:<programlisting>elems :: SuffixArray a -> V.Vector (V.Vector a)
elems (SuffixArray d i) = V.map vecAt i
    where vecAt idx = V.drop idx d</programlisting></para>
        <para>Note that we can use <function>Data.Vector.map</function> in this case, since it maps
            a function over all elements of vector, returning a
            vector:<screen>*Main> <userinput>:type Data.Vector.map</userinput>
Data.Vector.map :: (a -> b) -> V.Vector a -> V.Vector b</screen></para>
        <para><emphasis role="italic">Note: if you have a computer science background, you might
                want to skip the next paragraphs.</emphasis></para>
        <para>To be able to count the number of occurrences of an n-gram in the suffix array, we
            need to locate the n-gram in the suffix array first. We could just traverse the array
            from beginning to the end, comparing each element to the n-gram that we are looking for.
            However, this is not very inefficient. During every search step, we exclude just one
            element. For instance, if we have the numbers 0 to 9 and have to find the location of
            the number 7, the first search step would just exclude the number 0, leaving eight
            potential candidates (<xref linkend="fig-linear-search-step"/>).<figure
                xml:id="fig-linear-search-step">
                <title>Linear search step</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="../images/linear-search.svg" width="15em"/>
                    </imageobject>
                </mediaobject>
            </figure></para>
        <para>However, if we know that the vector of numbers is sorted, we can devise a more
            intelligent strategy. As a child, you probably played number guessing games. In one
            variant of the game, you would guess a number, and the person knowing the correct number
            would shout "smaller", "larger" or "correct". Being a smart kid, you would probably not
            start guessing at 1 if you had to guess a number between 1 and 100. Usually, you'd start
            somewhere halfway the range (say 50), and continue halfway the 1..50 or 51..100 range if
            the number was smaller or greater than 50.</para>
        <para>The same trick can be applied when searching a sorted vector. If you compare a value
            to the element in the middle, you remove cut half of the search space (if initial guess
            was not correct). This procedure is called a <emphasis role="italic">binary
                search</emphasis>. For instance, <xref linkend="fig-binary-search"/> shows the first
            search step when applying a binary search to the example in <xref
                linkend="fig-linear-search-step"/>.<figure xml:id="fig-binary-search">
                <title>Binary search step</title>
                <mediaobject>
                    <imageobject>
                        <imagedata fileref="../images/binary-search.svg" width="15em"/>
                    </imageobject>
                </mediaobject>
            </figure></para>
        <para>The performance of binary search compared to linear search should not be
            underestimated: the time of a linear search grows linearly with the number of elements
            (yes, we like pointing out the obvious), while time of a binary search grows
            logarithmically. Suppose that we have a sorted vector of 1048576 elements, a linear
            search would at most take 1048576 steps, while a binary search takes at most 20 steps.
            Pretty impressive right?</para>
        <para>On to our binary search function. <function>binarySearchByBounded</function> finds the
            index of an element in a <type>Vector</type>, wrapped in <type>Maybe</type>. If the
            element has multiple occurrences in the <type>Vector</type>, just one index is returned.
            If the element is not in the <type>Vector</type>, <type>Nothing</type> is
            returned.<programlisting>binarySearchByBounded :: (Ord a) => (a -> a -> Ordering) -> V.Vector a ->
                         a -> Int -> Int -> Maybe Int
binarySearchByBounded cmp v e lower upper
    | V.null v      = Nothing
    | upper &lt; lower = Nothing
    | otherwise     = case cmp e (v V.! middle) of
                        LT -> binarySearchByBounded cmp v e lower (middle - 1)
                        EQ -> Just middle
                        GT -> binarySearchByBounded cmp v e (middle + 1) upper
    where middle    = (lower + upper) `div` 2</programlisting></para>
        <para><function>binarySearchByBounded</function> takes a host of arguments: a comparison
            function, the (sorted) vector (<varname>v</varname>, the element to search for
                (<varname>e</varname>), and lower (<varname>lower</varname>) and upper bound
                (<varname>upper</varname>) indices of the search space. The function works just like
            we described above. First we have to find the middle of the current search space, we do
            this by averaging the upper and lower bounds and binding it to
            <varname>middle</varname>. We then compare the element at index
                <varname>middle</varname> in the vector to <varname>e</varname> . If both are equal
                (<type>EQ</type>), then we are done searching, and return <code>Just middle</code>
            as the index. If <varname>e</varname> is smaller than (<type>LT</type>) the current
            element, we search in the lower half of the search space
                (<varname>lower</varname>..<varname>middle</varname>-1). If <varname>e</varname> is
            greater than (<type>GT</type>) the current element, we search in the upper half of the
            search space (<varname>middle</varname>+1..<varname>upper</varname>). If
                <varname>e</varname> does not occur in the search space, <varname>upper</varname>
            will become smaller than <varname>lower</varname> when we have exhausted the search
            space.</para>
        <para>Let's define two convenience functions to make binary searches a bit
            simpler:<programlisting>binarySearchBounded :: (Ord a) => V.Vector a -> a -> Int -> Int -> Maybe Int
binarySearchBounded = binarySearchByBounded compare

binarySearchBy :: (Ord a) => (a -> a -> Ordering) -> V.Vector a -> a ->
                  Maybe Int
binarySearchBy cmp v n = binarySearchByBounded cmp v n 0 (V.length v - 1)

binarySearch :: (Ord a) => V.Vector a -> a -> Maybe Int
binarySearch v e = binarySearchBounded v e 0 (V.length v - 1)</programlisting></para>
        <para><function>binarySearchBounded</function> calls
                <function>binarySearchByBounded</function>, using Haskell's standard compare
            function. <function>binarySearchBy</function> calls
                <function>binarySearchByBounded</function>, binding the upper and lower bounds to
            the lowest index of the array (0) and the highest (the size of the <type>Vector</type>
            minus one). Finally, <function>binarySearch</function> combines the functionality of
                <function>binarySearchBounded</function> and <function>binarySearchBy</function>,
            Let's give the binary search functionality a
            try:<screen>*Main> binarySearch (V.fromList [1,2,3,5,7,9]) 9
Just 1
*Main> binarySearch (V.fromList [1,2,3,5,7,9]) 10
Just 5
*Main> binarySearch (V.fromList [1,2,3,5,7,9]) 10</screen></para>
        <para>Great! Let's make a step in between, returning to suffix arrays. Say that you would
            want to write a <function>contains</function> function that returns <type>True</type> if
            an n-gram is in the suffix array, or <type>False</type> otherwise. Easy right? Your
            first attempt may be something
            like:<screen>*Main> <userinput>let corpus = ["to","be","or","not","to","be"]</userinput>
*Main> binarySearch (elems $ fromList corpus) $ Data.Vector.fromList ["or","not", "to", "be"]
Just 3</screen></para>
        <para>Nice, right? But try this
            example:<screen>*Main> <userinput>binarySearch (elems $ fromList corpus) $ Data.Vector.fromList ["or","not"]</userinput>
Nothing</screen></para>
        <para>You can almost hear the commentator of <emphasis role="italic">Roger Wilco and the
                Time Rippers</emphasis> in the background, right? Right! Of course, the element that
            we are looking for contains the n-gram of the maximum length ("or not to be"). That is
            why the first example worked, while the second did not. So, we have to apply the binary
            search to something that only contains bigrams in this
            case:<screen>*Main> <userinput>:{</userinput>
*Main| <userinput>binarySearch</userinput>
*Main|   <userinput>(Data.Vector.map (Data.Vector.take 2) $ elems $ fromList corpus) $</userinput>
*Main|   <userinput>Data.Vector.fromList ["or","not"]</userinput>
*Main| <userinput>:}</userinput>
Just 3</screen></para>
        <para>That did the trick. Writing the contains function is now
            simple:<programlisting>contains :: Ord a => SuffixArray a -> V.Vector a -> Bool
contains s e = case binarySearch (restrict eLen s) e of
                 Just _  -> True
                 Nothing -> False
    where eLen = V.length e
          restrict len = V.map (V.take len) . elems</programlisting></para>
        <para>To find the frequency of an element in a <type>Vector</type>, we have to do a bit more
            than locating one instance of that element. One first intuition could be to find the
            element, and scan upwards and downwards to find how many instances of the element there
            are in the <type>Vector</type>. However, there could be millions of such elements. Doing
            a linear search is, again, not very efficient. So, we should apply a binary search, but
            not just to find one instance of the element, but specifically the first and the
            last.</para>
        <para>Such search functions are very comparable to the
                <function>binarySearchByBounds</function> function that we wrote earlier. Let's
            start with finding the first index in the <type>Vector</type> where a specified element
            occurs. Suppose that we do a binary search again: if the element in the middle of our
            search space is greater than the element, we want to continue searching in the lower
            half of the search space. If the element in the middle is smaller than the element, we
            want to continue searching in the upper half of the search space. If the middle is
            however equal to the element, we do not stop searching, but continue searching the lower
            half. We still keep the element that was equal in the search space, since it may have
            been the only instance of that element. This gives us the following
                <function>lowerBoundByBounds</function> function and corresponding
            helpers:<programlisting>lowerBoundByBounds :: Ord a => (a -> a -> Ordering) -> V.Vector a -> a ->
                      Int -> Int -> Maybe Int
lowerBoundByBounds cmp v e lower upper
    | V.null v = Nothing
    | upper == lower = case cmp e (v V.! lower) of
                         EQ -> Just lower
                         _  -> Nothing
    | otherwise = case cmp e (v V.! middle) of
                    GT -> lowerBoundByBounds cmp v e (middle + 1) upper
                    _  -> lowerBoundByBounds cmp v e lower middle
    where middle = (lower + upper) `div` 2

lowerBoundBounds :: Ord a => V.Vector a -> a -> Int -> Int -> Maybe Int
lowerBoundBounds = lowerBoundByBounds compare

lowerBoundBy :: Ord a => (a -> a -> Ordering) -> V.Vector a -> a -> Maybe Int
lowerBoundBy cmp v e = lowerBoundByBounds cmp v e 0 (V.length v - 1)

lowerBound :: Ord a => V.Vector a -> a -> Maybe Int
lowerBound = lowerBoundBy compare</programlisting></para>
        <para>Searching the last index in the <type>Vector</type> where the element occurs, follows
            a comparable procedure. We search as normal, however if the element is equal to the
            middle we search the upper half of the search space including the element that we found
            to be equal. Give the floor to <function>upperBoundByBounds</function> and
            helpers:<programlisting>upperBoundByBounds :: Ord a => (a -> a -> Ordering) -> V.Vector a -> a ->
                      Int -> Int -> Maybe Int
upperBoundByBounds cmp v e lower upper
    | V.null v       = Nothing
    | upper &lt;= lower = case cmp e (v V.! lower) of
                         EQ -> Just lower
                         _  -> Nothing
    | otherwise      = case cmp e (v V.! middle) of
                         LT -> upperBoundByBounds cmp v e lower (middle - 1)
                         _  -> upperBoundByBounds cmp v e middle upper
    where middle     = ((lower + upper) `div` 2) + 1

upperBoundBounds :: Ord a => V.Vector a -> a -> Int -> Int -> Maybe Int
upperBoundBounds = upperBoundByBounds compare

upperBoundBy :: Ord a => (a -> a -> Ordering) -> V.Vector a -> a -> Maybe Int
upperBoundBy cmp v e = upperBoundByBounds cmp v e 0 (V.length v - 1)

upperBound :: Ord a => V.Vector a -> a -> Maybe Int
upperBound = upperBoundBy compare</programlisting></para>
        <para>Note that we add one to the middle in this case. This is to avoid landing in an
            infinite recursion when <varname>middle</varname> is <varname>lower</varname> plus one,
            and the element is larger than or equal to the element at <varname>middle</varname>.
            Under those circumstances, <varname>lower</varname> and <varname>upper</varname> would
            be unchanged in the next recursion.</para>
        <para>Great. I guess you will now be able to write that function in terms of
                <function>lowerBoundByBounds</function> and
            <function>upperBoundByBounds</function>:<programlisting>frequencyByBounds :: Ord a => (a -> a -> Ordering) -> V.Vector a -> a ->
                     Int -> Int -> Maybe Int
frequencyByBounds cmp v e lower upper = do
  lower &lt;- lowerBoundByBounds cmp v e lower upper
  upper &lt;- upperBoundByBounds cmp v e lower upper
  return $ upper - lower + 1

frequencyBy :: Ord a => (a -> a -> Ordering) -> V.Vector a -> a ->
               Maybe Int
frequencyBy cmp v e = frequencyByBounds cmp v e 0 (V.length v - 1)

frequencyBounds :: Ord a => V.Vector a -> a -> Int -> Int -> Maybe Int
frequencyBounds = frequencyByBounds compare

frequency :: Ord a => V.Vector a -> a -> Maybe Int
frequency = frequencyBy compare</programlisting>This
            function works as
            expected:<screen>*Main> <userinput>frequency (V.fromList [1,3,3,4,7,7,7,10]) 7</userinput>
Just 3
*Main> <userinput>frequency (V.fromList [1,3,3,4,7,7,7,10]) 5</userinput>
Nothing</screen></para>
        <para>We can use this with our suffix array
            now:<screen>*Main> <userinput>let corpus = ["to","be","or","not","to","be"]</userinput>
*Main> <userinput>let sa = fromList corpus</userinput>
*Main> <userinput>containsWithFreq sa $ Data.Vector.fromList ["not"]</userinput>
Just 2
*Main> <userinput>containsWithFreq sa $ Data.Vector.fromList ["not"]</userinput>
Just 1
*Main> <userinput>containsWithFreq sa $ Data.Vector.fromList ["jazz","is","not","dead"]</userinput>
Nothing
*Main> <userinput>containsWithFreq sa $ Data.Vector.fromList ["it","just","smells","funny"]</userinput>
Nothing</screen></para>
        <sect2>
            <title>Exercises</title>
            <para>
                <orderedlist>
                    <listitem>
                        <para>Write a function <function>mostFrequentNgram</function> with the
                            following type
                            signature:<programlisting>mostFrequentNgram :: Ord a => SuffixArray a -> Int -> Maybe (V.Vector a, Int)</programlisting></para>
                        <para>This function extracts the most frequent n-gram from a suffix array,
                            where the suffix array and <emphasis role="italic">n</emphasis> are
                            given as arguments. The function should continue a pair of the n-gram
                            and the frequency as a typle wrapped in Maybe. If no n-gram could be
                            extracted (for instance, because the suffix array contains to few
                            elements), return <type>Nothing</type>.</para>
                    </listitem>
                    <listitem>
                        <para>Use <function>mostFrequentNgram</function> to find the most frequent
                            bigram and trigram in the Brown corpus.</para>
                    </listitem>
                    <listitem>
                        <para><type>frequencyByBounds</type> is not as efficient as it could be: it
                            performs a search of the full <type>Vector</type> twice. A more
                            efficient solution would be to narrow down the search space until the
                            first match is found, and then using
                                <function>lowerBoundByBounds</function> and
                                <function>upperBoundByBounds</function> to search the lower and
                            upper half of the search space. Modify
                                <function>frequencyByBounds</function> to use this
                            methodology.</para>
                    </listitem>
                </orderedlist>
            </para>
        </sect2>
    </sect1>
    <sect1 xml:id="chap-ngrams-markov-models">
        <title>Markov models</title>
        <para>At the beginning of this chapter we mentioned that n-grams can be exploited to model
            language. While they may not be so apt as computational grammars, n-grams do encode some
            syntax albeit local. For instance, consider the following to phrases:<itemizedlist>
                <listitem>
                    <para>the plan was</para>
                </listitem>
                <listitem>
                    <para>* plan the was</para>
                </listitem>
            </itemizedlist></para>
        <para>The first phrase is clearly grammatical, while the second is not. We could neatly
            encode this using a syntax rule, but we could also count how often both combinations of
            words occur in a large text corpus. The first phrase is likely to occur a few times,
            while the second phrase is not likely to occur. Or more formally, the probability that
            we encounter <emphasis>this plan was</emphasis> occurs in a random text is higher than
            the probability that <emphasis>plan this was</emphasis> occurs:<equation>
                <xi:include href="eq-prob-phrases.mml"/>
            </equation></para>
        <para>Of course, we could also try to find the most grammatical of two sentences by
            comparing the probabilities of the sentences. So, if we have a sentence consisting of
            the words w<subscript>0..n</subscript> and a sentence consisting of the words
                v<subscript>0..m</subscript> that both aim to express the same meaning and the
            following is true:<equation>
                <xi:include href="eq-prob-sentences.mml"/>
            </equation></para>
        <para>We could conclude that the use of w<subscript>0..n</subscript> is preferred over
                v<subscript>0..m</subscript>, since w<subscript>0..n</subscript> is either more
            grammatical or more fluent. So, how do we estimate the probability of such a sentence?
            Good question, at first sight it seems pretty easy. We simply count how often a sentence
            occurs in a text corpus, and divide it by the total number of sentences in a corpus:<equation>
                <title>Estimating the probability of a sentence</title>
                <xi:include href="eq-prob-sentence.mml"/>
            </equation></para>
        <para>Here <emphasis>C</emphasis> is a counter function, and <emphasis>N</emphasis> is the
            total number of sentences in a corpus. While this is a theoretically sound method for
            estimating the probability, it does not work in practice. As ingenious as human language
            is, we can construct an infinite number of grammatical sentences. So, to be able to
            estimate the probability we would need an infinite text corpus, since not every
            grammatical sentence will occur in a finite corpus. Given that we only have a finite
            text corpus, we would simply give a probability of zero to many perfectly grammatical
            sentences. We encounter so-called <emphasis>data sparseness</emphasis>. This is nasty,
            because it interferes with our goal to compare the quality of sentences.</para>
        <para>Fortunately for us, some smart people have thought about this problem, and came up
            with a pretty elegant solution (or `workaround' as we programmers like call it). To get
            to the solution, we have to make an intermediate step. This intermediate step does not
            immediately solve our problem, but sets the stage for the solution. We can decompose the
            probability of a sentence <emphasis>p(w<subscript>0..n</subscript>)</emphasis> into a
            series of conditional probabilities:<equation>
                <title>The probability of a sentence as a Markov chain</title>
                <xi:include href="eq-prob-markov-chain.mml"/>
            </equation></para>
        <para>Before this gets too confusing, let's write down how you would estimate the
            probability of the sentence <emphasis>Colorless green ideas sleep furiously</emphasis>
            in this manner: <emphasis>p(Colorless) p(green|Colorless) p(ideas|Colorless green)
                p(sleep|Colorless green ideas) p(furiously|Colorless green ideas
            sleep)</emphasis>.</para>
        <para>Simple huh? Now, how do we estimate such a conditional probability? Formally, this is
            estimated in the following manner:<equation>
                <xi:include href="eq-prob-chain-conditional.mml"/>
            </equation></para>
        <para>That is all nice and dandy, but as you may already see, this does not solve our
            problem with data sparseness. For if we want to estimate <emphasis>p(furiously|Colorless
                green ideas sleep)</emphasis>, we need counts of <emphasis>Colorless green ideas
                sleep</emphasis> and <emphasis>Colorless green ideas sleep furiously</emphasis>.
            Even if we decompose the probability of a sentence into conditional probabilities, we
            need counts for the complete sentence.</para>
        <para>However, if we look at the conditional probability of a word, the following often
            holds:<equation>
                <title>Approximation using the Markov assumption</title>
                <xi:include href="eq-prob-chain-approximation.mml"/>
            </equation></para>
        <para>More formally, this is a process with the <emphasis>Markov property</emphasis>:
            prediction of the next <emphasis role="italic">state</emphasis> (word) is only dependent
            on the current state. Of course, we can easily calculate our revised conditional
            probability:<equation>
                <title>The conditional probability of a word using the Markov assumption</title>
                <xi:include href="eq-prob-bigram-conditional.mml"/>
            </equation></para>
        <para>That spell worked! We only need counts of... unigrams (1-grams) and bigrams to
            estimate the conditional probability of each word. This is a <emphasis>bigram language
                model</emphasis>, which we can use to estimate to probability of a
            sentence:<equation>
                <title>The probability of a sentence using a bigram model</title>
                <xi:include href="eq-prob-sentence-bigram.mml"/>
            </equation></para>
        <para>In practice it turns out that knowledge of previous states can help a bit in
            estimating the conditional probability of a word. However, if we increase the context
            too much, we run into the same data sparseness problems that we solved by drastically
            cutting the context. The consensus is that for most applications a trigram language
            model provides a good trade-off between data availability and estimator quality.</para>
        <sect2>
            <title>Implementation</title>
            <para>The implementation of a bigram Markov model in Haskell should now be trivial. If
                we have a frequency map of unigrams and bigrams of the type <type>(Ord a, Integral
                    n) => Map [a] n</type>, we could write a function that calculates <inlineequation>
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <mrow>
                            <mi>p</mi>
                            <mo>(</mo>
                            <msub>
                                <mi>w</mi>
                                <mi>n</mi>
                            </msub>
                            <mo>|</mo>
                            <msub>
                                <mi>w</mi>
                                <mrow>
                                    <mi>n</mi>
                                    <mo>-</mo>
                                    <mi>1</mi>
                                </mrow>
                            </msub>
                            <mo>)</mo>
                        </mrow>
                    </math>
                </inlineequation>, or more generally <inlineequation>
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <mrow>
                            <mi>p</mi>
                            <mo>(</mo>
                            <msub>
                                <mi>state</mi>
                                <mi>n</mi>
                            </msub>
                            <mo>|</mo>
                            <msub>
                                <mi>state</mi>
                                <mrow>
                                    <mi>n</mi>
                                    <mo>-</mo>
                                    <mi>1</mi>
                                </mrow>
                            </msub>
                            <mo>)</mo>
                        </mrow>
                    </math>
                </inlineequation>:<programlisting>import qualified Data.Map as M
import Data.Maybe (fromMaybe)

pTransition :: (Ord a, Integral n, Fractional f) =>
  M.Map [a] n -> a -> a -> f
pTransition ngramFreqs state nextState = fromMaybe 0.0 $ do
  stateFreq &lt;- M.lookup [state] ngramFreqs
  transFreq &lt;- M.lookup [state, nextState] ngramFreqs
  return $ (fromIntegral transFreq) / (fromIntegral stateFreq)</programlisting></para>
            <para>Now we write a function that extracts all bigrams, calculates the transition
                probabilities and takes the product of the transition
                probabilities:<programlisting>pMarkov :: (Ord a, Integral n, Fractional f) =>
  M.Map [a] n -> [a] -> f
pMarkov ngramFreqs =
  product . map (\[s1,s2] -> pTransition ngramFreqs s1 s2) . ngrams 2</programlisting></para>
            <para>This function is straightforward, except perhaps the <function>product</function>
                function. <function>product</function> calculates the product of a
                list:<screen>Prelude> <userinput>:type product</userinput>
product :: Num a => [a] -> a
Prelude> <userinput>product [1,2,3]</userinput>
6</screen></para>
        </sect2>
    </sect1>
</chapter>
