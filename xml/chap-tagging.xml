<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbookxi.rng" type="xml"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="chap-tagging">
    <title>Part of speech tagging</title>
    <sect1 xml:id="sec-tagging-intro">
        <title>Introduction</title>
        <para>In the last episode, you have seen how n-gram language models can be used to model
            structure of language, purely based on words. In this chapter, we will make a further
            abstraction and will try to find proper <emphasis role="italic">part of speech
                tags</emphasis> (also named <emphasis role="italic">morphosyntactic tags</emphasis>)
            for words. Part of speech tags give relevant information about the role of a word in its
            narrow context. It may also provide information about the inflection of a word. POS tags
            are a valuable part of a language processing pipeline, they provide useful information
            to other components such as a parser or a named-entity recognizer.</para>
        <para>There is no such thing as a standard set of part of speech tags (let's call them 'POS
            tags' from now on). Just like programming languages, text editors, and operating
            systems, the tag set that people use depends on the task at hand and taste. For our
            purposes, we will use the Brown tag set<footnote>
                <para>A full description of the Brown tag set can be found at: <link
                        xlink:href="http://www.scs.leeds.ac.uk/ccalas/tagsets/brown.html"
                        >http://www.scs.leeds.ac.uk/ccalas/tagsets/brown.html</link></para>
            </footnote>.</para>
        <para>This is a sentence from the Brown corpus that is annotated with tags:</para>
        <para><emphasis role="italic">A/AT similar/JJ resolution/NN passed/VBD in/IN the/AT
                Senate/NN by/IN a/AT vote/NN of/IN 29-5/CD ./.</emphasis></para>
        <para>The notation here is very simple: as our previous fragments of the Brown corpus the
            sentence is pre-tokenized. However, each word is amended by a POS tag that indicate the
            role of the world. For instance, the word 'a' is an <emphasis role="italic"
                >article</emphasis>, 'similar' an <emphasis role="italic">adjective</emphasis>, and
            'resolution' a <emphasis role="italic">singular common noun</emphasis>.</para>
        <para>Corpora, such as the Brown corpus only provide POS tags for a small amount of
            sentences that occur in corpus. Being a working programmer, you will deal with new data
            that does not occur in the Brown corpus. Now, wouldn't it be nice to have a set of
            functions that could add POS tags to untagged data? Software that performs this task is
            called a POS tagger or morphosyntactic tagger, and this is exactly the thing we will
            build in this chapter.</para>
        <sect2>
            <title>Exercises</title>
            <itemizedlist>
                <listitem>
                    <para>In the data provided with this book, you will find the file
                            <filename>brown-pos-train.txt</filename>. Open this file with a text
                        file viewer or text editor, and look at the five first sentences. Try to
                        find out what the tags mean using the description of the Brown tag
                        set.</para>
                </listitem>
            </itemizedlist>
        </sect2>
    </sect1>
    <sect1 xml:id="sec-tagging-frequency">
        <title>Frequency-based tagging</title>
        <para>In one of the simplest forms of tagging, we just assign the most frequent POS tag for
            a token in the training data to a token in untagged data. That's right, the most
            frequent tag, because a token can have more than one tag. Consider the following two
            sentences:</para>
        <itemizedlist>
            <listitem>
                <para>I wouldn't <emphasis role="bold">trust</emphasis> him.</para>
            </listitem>
            <listitem>
                <para>He put money in the family <emphasis role="bold">trust</emphasis>.</para>
            </listitem>
        </itemizedlist>
        <para>Both sentences contain the word 'trust'. However, 'trust' has different roles in
            different roles in both sentences. In the first sentence 'trust' is a verb, in the
            second sentence it is a noun. So, for many tokens we will have the choice of multiple
            tags. If we tag the token with the most frequent tag, we will frequently tag tokens
            incorrectly, but it is a first step.</para>
        <para>To ease handling of tokens and tags, we will make type aliases for tokens and tags and
            define a new datatype for training instances, aptly named <type>TrainingInstance</type>:<programlisting>type Token = String
type Tag = String

data TrainingInstance = TrainingInstance Token Tag
                        deriving Show</programlisting></para>
        <para>The <type>Token</type> and <type
                >Tag</type> aliases will allow us to write clean function signatures. The
                <type>TrainingInstance</type> data type has only one
            constructor, <type>TrainingInstance</type>. The data type derives
            from the <type>Show</type> typeclass, which allows us to get a
                <type>String</type> representation of an instance<footnote>
                <para>This <type>String</type> representation is also used by
                        <command>ghci</command> to print the value of a <type>TrainingInstance</type>.</para>
            </footnote>. We can use this constructor to create training
            instances:<screen>*Main> <userinput>TrainingInstance "the" "AT"</userinput>
TrainingInstance "the" "AT"
*Main> <userinput>TrainingInstance "pony" "NN"</userinput>
TrainingInstance "pony" "NN"</screen></para>
        <para>Since our first POS tagger is trained purely on tokens and tags, and requires no
            sentencial information, the corpus will be represented as a list of <type>TrainingInstance</type>. Since we can use the <function>words</function> function to tokenize the corpus, the task at hand is
            to convert a list of strings of the format "token/tag" to a list of <type>TrainingInstance</type>. This is done by splitting the <type
                >String</type> on the forward slash character (/). We can use the
                <function>break</function> function to break the string on the first element for
            which the supplied function is true. For
            instance:<screen>Prelude> <userinput>break (== '/') "the/AT"</userinput>
("the","/AT")</screen></para>
        <para>This is a good start, we would only have to chop off the first character of the second
            element in the tuple. However, there is another problem: although a tag can never
            contain a slash, a token can. Consequently, we should break the string on the last
            slash, rather than the first. A cheap solution to this problem could be to reverse the
            string, applying <function>break</function>, and then reversing the results again. We
            will take a more sophisticated route, and write our own
            function:<programlisting>rsplit :: Eq a => a -> [a] -> ([a], [a])
rsplit sep l = let (ps, xs, _) = rsplit_ sep l in
               (ps, xs)

rsplit_ :: Eq a => a -> [a] -> ([a], [a], Bool)
rsplit_ sep = foldr (splitFun sep) ([], [], False)
    where splitFun sep e (px, xs, True) = (e:px, xs, True)
          splitFun sep e (px, xs, False)
                   | e == sep = (px, xs, True)
                   | otherwise = (px, e:xs, False)</programlisting></para>
        <para>The core business happens in the <function>rsplit_</function> function, it splits a
            list in the part before the last instance of <varname>sep</varname> (the
            prefix) and the part after (the suffix). It does this by folding over the input list
            from right to left. The accumulator is a tuple that holds the prefix list, the suffix
            list, and a <type>Bool</type> indicating whether the separator was
            encountered. The function provided to the fold acts upon this <type
                >Bool</type>:<itemizedlist>
                <listitem>
                    <para>If the <type>Bool</type> is <type
                            >True</type>, the separator was seen, and the current element is
                        added to the prefix list.</para>
                </listitem>
                <listitem>
                    <para>If the <type>Bool</type> is <type
                            >False</type>, the separator was not seen yet. If the current
                        element is equal to the separator, the <type
                            >Bool</type> is changed to <type>True</type>
                        to indicate that all remaining elements should be added to the prefix list.
                        Otherwise, the element is added to the suffix list.</para>
                </listitem>
            </itemizedlist><function>rsplit</function> is just a tiny wrapper around
                <function>rsplit_</function> that returns a binary tuple with just the prefix and
            suffix lists. The <function>rsplit</function> function works as
            intended:<screen>*Main> <userinput>rsplit '/' "the/AT"</userinput>
("the","AT")
*Main> <userinput>rsplit '/' "a/b/TEST"</userinput>
("a/b","TEST")</screen></para>
        <para>We are now able to get the necessary data out of a <type
                >String</type> containing a token and a tag. We can simply construct a training
            instance by converting the
            tuple:<programlisting>toTrainingInstance :: String -> TrainingInstance
toTrainingInstance s = let (token, tag) = rsplit '/' s in
TrainingInstance token tag</programlisting></para>
        <!-- XXX - Read/Show is not really appropriate here, keep for later...
            <para>Rember that we let <emphasis
            role="italic">TrainingInstance</emphasis> derive from <emphasis role="italic"
            >Show</emphasis>? The typeclass <emphasis role="italic">Read</emphasis> is the
            converse of <emphasis role="italic">Show</emphasis> - data types implementing <emphasis
            role="italic">Read</emphasis> provide a <function>read</function>function that
            constructs an value from a <emphasis role="italic"
            >String</emphasis>:<screen>*Main> :type read
            read :: (Read a) => String -> a</screen></para>
            <para>To give an example, you can use <function>read</function> to read an <emphasis
            role="italic">Int</emphasis> or a <emphasis role="italic">Float</emphasis> from a
            <emphasis role="italic"
            >String</emphasis>:<screen>*Main> read "2" :: Int
            2
            *Main> read "2" :: Float
            2.0
            *Main> read "2.3" :: Float
            2.3</screen></para>
            <para>Wouldn't it be nice if we can convert <emphasis role="italic"
            >TrainingInstances</emphasis> from and to the corpus <emphasis role="italic"
            >String</emphasis> representation by implementing the <emphasis role="italic"
            >Show</emphasis> and <emphasis role="italic">Read</emphasis> typeclasses? With the
            functions that we have now, this turns out to be
            doable:<programlisting>data TrainingInstance = TrainingInstance Token Tag
            
            instance Show TrainingInstance where
            show i = let (TrainingInstance token tag) = i in
            token ++ "/" ++ tag
            
            instance Read TrainingInstance where
            readsPrec _ s = let (word, tag) = rsplit '/' s in
            [(TrainingInstance word tag, "")]</programlisting></para>
            <para>First, we removed the <emphasis role="italic">deriving</emphasis> clause from the
            definition of <type>TrainingInstance</type>, since we want to
            define what it means for <type>TrainingInstance</type> to be of
            the <emphasis role="italic">Show</emphasis> typeclass ourselves. To do this, we use the
            <emphasis role="italic">instance</emphasis> keyword to add <emphasis role="italic"
            >TrainingInstance</emphasis> as an instance of the <emphasis role="italic"
            >Show</emphasis> typeclass. We provide our own <function>show</function> function
            that simply concatenates the token and the tag, separated by a forward slash. The
            <emphasis role="italic">instance</emphasis> definition of <emphasis role="italic"
            >Read</emphasis> is a bit more complicated. It gives the opportunity to handle
            operator precedence (the first argument to <function>readsPrec</function>), and we are
            supposed to parse only the necessary part of the input.</para>
        -->
        <para>Why not see how we are doing, and get the ten first training instances of the Brown
            corpus?<screen>*Main> <userinput>h &lt;- IO.openFile "brown-pos-train.txt" IO.ReadMode</userinput>
*Main> <userinput>c &lt;- IO.hGetContents h</userinput>
*Main> <userinput>take 10 $ map toTrainingInstance $ words c</userinput>
[TrainingInstance "The" "AT",TrainingInstance "Fulton" "NP",
  TrainingInstance "County" "NN",TrainingInstance "Grand" "JJ",
  TrainingInstance "Jury" "NN",TrainingInstance "said" "VBD",
  TrainingInstance "Friday" "NR",TrainingInstance "an" "AT",
  TrainingInstance "investigation" "NN",TrainingInstance "of" "IN"]</screen></para>
        <para>Allright! That's indeed our corpus in beautified format. The next step is to traverse
            this corpus, registering for each word with which tag it occured (and how often). For
            this we write the <function>tokenTagFreq</function>
            function:<programlisting>import qualified Data.List as L
import qualified Data.Map as M

tokenTagFreqs :: [TrainingInstance] -> M.Map Token (M.Map Tag Int)
tokenTagFreqs = L.foldl' countWord M.empty
    where
      countWord m (TrainingInstance token tag) = 
          M.insertWith (countTag tag) token (M.singleton tag 1) m
      countTag tag _ old = M.insertWith
          (\newFreq oldFreq -> oldFreq + newFreq) tag 1 old</programlisting></para>
        <para>This function is very comparable to the <function>countElem</function> function we saw
            earlier, the primary difference being that we have to handle two levels of maps. Every
                <type>Token</type> in the first <type
                >Map</type> is associated with a value that is itself a <type
                >Map</type> that maps <type>Tag</type>s to frequencies
                (<type>Int</type>). If we have not seen a particular <type>Token</type> yet, we will insert it to the map with the <type>Token</type> as the key, the value is a map with just one
            key/value: the <type>Tag</type> associated with the token and a
            frequency of one. If the <type>Token</type> was seen before, we
            will update the frequency of the associated <type>Tag</type>,
            setting it to one, if the <type>Tag</type> was never seen before
            with this token.</para>
        <para>Let us test <function>tokenTagFreqs</function> on the first ten training
            instances as
            well:<screen>*Main> <userinput>h &lt;- IO.openFile "brown-pos-train.txt" IO.ReadMode</userinput>
*Main> <userinput>c &lt;- IO.hGetContents h</userinput>
*Main> <userinput>tokenTagFreqs $ take 10 $ map toTrainingInstance $ words c</userinput>
fromList [("County",fromList [("NN",1)]),("Friday",fromList [("NR",1)]),
  ("Fulton",fromList [("NP",1)]),("Grand",fromList [("JJ",1)]),
  ("Jury",fromList [("NN",1)]),("The",fromList [("AT",1)]),
  ("an",fromList [("AT",1)]),("investigation",fromList [("NN",1)]),
  ("of",fromList [("IN",1)]),("said",fromList [("VBD",1)])]</screen></para>
        <para>It seems to work, but we cannot be sure until we have seen duplicatesand ambiguous
            tokens. You may want to play a little with larger corpus samples or artificial training
            data to confirm that <function>tokenTagFreqs</function> works as intended.</para>
        <para>The next thing we need for our first part of speech tagger is use the map defined by
                <function>tokenTagFreqs</function> to find the most frequent tag for a word. This is
            a typical mapping situation: for each key/value pair in the <type
                >Map</type>, we want to transform its value. The value was a <type>Map</type>, mapping <type>Tag</type> to
                <type>Int</type>, and we want the value to be a <type
                >Tag</type>, namely the most frequent <type
                >Tag</type>. There is also a <function>map</function> functions for <type
            >Map</type>:<screen>*Main> <userinput>:type Data.Map.map</userinput>
Data.Map.map :: (a -> b) -> M.Map k a -> M.Map k b</screen></para>
        <para><function>Data.Map.map</function> accepts some function to map every value in a
                <type>Map</type> to a new value. For getting the most frequent
                <type>Tag</type>, we have to fold over the inner map, storing
            the most frequent tag and its frequency in the accumulator. The <type
                >Data.Map</type> module provides the <function>foldlWithKey</function> to fold
            over keys and
            values:<screen>*Main> <userinput>:type Data.Map.foldlWithKey</userinput>
Data.Map.foldlWithKey :: (b -> k -> a -> b) -> b -> M.Map k a -> b</screen></para>
        <para>This looks like the usual suspect, however, the folding function takes an additional
            parameter. The folding function has the current accumulator, the current key, and the
            associated value as its arguments. Using these building blocks, we can construct the
                <function>tokenMostFreqTag</function>
            function:<programlisting>tokenMostFreqTag :: M.Map Token (M.Map Tag Int) -> M.Map Token Tag
tokenMostFreqTag = M.map mostFreqTag
    where
      mostFreqTag = fst . M.foldlWithKey maxTag ("NIL", 0)
      maxTag acc@(maxTag, maxFreq) tag freq
                 | freq > maxFreq = (tag, freq)
                 | otherwise = acc</programlisting></para>
        <para>The main function body uses <function>mostFreqTag</function> to get the most frequent
            tag for each token. <function>mostFreqTag</function> folds over all tokens and
            frequencies of a map associated with a token. The initial value of the accumulator is
            the dummy tag 'NIL'. The <function>maxTag</function> function that is used in the fold
            will replace the accumulator with the current tag and its frequency if its frequency is
            higher than the frequency of the tag in the accumulator. Otherwise, the tag in the
            accumulator is more frequent, and the accumulator is retained. After folding, we have
            the pair of the most frequent tag, and its frequency. We use the
                <function>fst</function> function to get the first element of this pair.</para>
        <para>You can craft some examples to check whether <function>tokenMostFreqTag</function>
            works as intended. For
            example:<screen>*Main> <userinput>tokenMostFreqTag $ tokenTagFreqs [TrainingInstance "a" "A",
  TrainingInstance "a" "B", TrainingInstance "a" "A"]</userinput>
fromList [("a","A")]</screen></para>
        <para>Combining <function>tokenTagFreqs</function> and <function>tokenMostFreqTag</function>
            we can make a simple function to train our first tagging model from a list of <type
            >TrainingInstance</type>:<programlisting>trainFreqTagger :: [TrainingInstance] -> M.Map Token Tag
trainFreqTagger = tokenMostFreqTag . tokenTagFreqs</programlisting></para>
        <para>Next up is the actual tagger: it simply looks up a token, returning the most frequent
            tag of a token. Since not all tags may be known, you may want to decide how to handle
            unknown tags. For now, we will just return the <function>Maybe
                Tag</function> type, allowing us to return <function
                >Nothing</function> in the case we do not know how to tag a word. We will define the
            function <function>freqTagWord</function> as a simple wrapper around
                <function>Data.Map.lookup</function>:<programlisting>freqTagWord :: M.Map Token Tag -> Token -> Maybe Tag
freqTagWord m t = M.lookup w t</programlisting></para>
        <para>We can now train our model from the Brown corpus, and tag some
            sentences:<screen>*Main> <userinput>h &lt;- IO.openFile "brown-pos-train.txt" IO.ReadMode</userinput>
*Main> <userinput>c &lt;- IO.hGetContents h</userinput>
*Main> <userinput>let model = trainFreqTagger $ map toTrainingInstance $ words c</userinput>
*Main> <userinput>map (freqTagWord model) ["The","cat","is","on","the","mat","."]</userinput>
[Just "AT",Just "NN",Just "BEZ",Just "IN",Just "AT",Just "NN",Just "."]
*Main> <userinput>map (freqTagWord model) ["That's","right",",","the","mascara",
  "snake",".","Fast","and","bulbous",".","Also","a","tinned","teardrop","."]</userinput>
[Just "DT+BEZ",Just "QL",Just ",",Just "AT",Just "NN",Just "NN",Just ".",
  Nothing,Just "CC",Nothing,Just ".",Just "RB",Just "AT",Nothing,
  Just "NN",Just "."]</screen></para>
        <para>Isn't that NLP for the working programmer? Not only did you learn about POS tagging,
            you built your own first POS tagger in just a few lines of Haskell code. In the next
            section we will be a bit more scientific, and focus on evaluation of taggers.</para>
    </sect1>
    <sect1 xml:id="sec-tagging-evaluation">
        <title>Evaluation</title>
        <para>Now that you wrote your first tagger, the question is how well it work. Not only to
            show it off to your colleague (it will do relatively well), but also to be able to see
            how future changes impact the performance of the tagger. To check the performance of the
            tagger, we will use an evaluation corpus. You should never evaluate a natural language
            processing component on the training data, because it is easy to perform well on seen
            data. Suppose that you wrote a tagger that just remembered the training corpus exactly.
            This tagger would tag every word correctly, but it will behave badly on unseen
            data.</para>
        <para>For evaluating our taggers, we will use another set of sentences from the Brown
            corpus. These annotated sentences are provided in
                <filename>brown-pos-test.txt</filename>. Since file has the same format as
                <filename>brown-pos-train.txt</filename>, it can also be read as a list of <type>TrainingInstance</type>.</para>
        <para>To evaluate a POS tagger, we will write a function that takes a tagging function
                (<type>Word -> Maybe Tag</type>) as its first argument and a
            training corpus as its second argument. It should then return a tuple with the total
            number of tokens in the corpus, the number of tokens that were tagged correctly, and the
            number of tokens for which the tagger did not provide an analysis (returned <type
                >Nothing</type>). This is the <function>evalTagger</function>
            function:</para>
        <programlisting>evalTagger tagFun = L.foldl' eval (0, 0, 0)
    where
      eval (n, c, u) (TrainingInstance token correctTag) =
          case tagFun token of
            Just tag -> if tag == correctTag then
                             (n+1, c+1, u)
                         else
                             (n+1, c, u)
            Nothing  -> (n+1, c, u+1)</programlisting>
        <para>The function is pretty simple, it folds over all instances in the evaluation data. The
            counts are incremented in the following manner:<itemizedlist>
                <listitem>
                    <para>If the tagger returned a tag for the current token, we have two options:<itemizedlist>
                            <listitem>
                                <para>The tagger picked the correct tag. We increment the number of
                                    tokens and the number of correct tags by one.</para>
                            </listitem>
                            <listitem>
                                <para>The tagger picked an incorrect tag. We only increment the
                                    number of tokens by one.</para>
                            </listitem>
                        </itemizedlist></para>
                </listitem>
                <listitem>
                    <para>The tagger returned no tag for the current token. We increment the number
                        of tokens and the number of untagged tokens by one.</para>
                </listitem>
            </itemizedlist></para>
        <para>Time to evaluate your first tagger!</para>
        <screen>*Main> <userinput>h &lt;- IO.openFile "brown-pos-train.txt" IO.ReadMode</userinput>
*Main> <userinput>c &lt;- IO.hGetContents h</userinput>
*Main> <userinput>let model = trainFreqTagger $ map toTrainingInstance $ words c</userinput>
*Main> <userinput>i &lt;- IO.openFile "brown-pos-test.txt" IO.ReadMode</userinput>
*Main> <userinput>d &lt;- IO.hGetContents i</userinput>
*Main> <userinput>evalTagger (freqTagWord model) $ map toTrainingInstance $ words d</userinput>
(272901,239230,11536)</screen>
        <para>Those are quite impressive numbers for a first try, <emphasis role="italic">239230 /
                272901 * 100% = 87.66%</emphasis> of the tokens were tagged and tagged correctly. Of
            the remaining 12.34% of the tokens, <emphasis role="italic">11536 / 272901 * 100% =
                4.23%</emphasis> of the words were not known. This means that we tagged <emphasis
                role="italic">239230 / (272901 - 11536) * 100% = 91.53%</emphasis> of the words
            known to our model correctly.</para>
        <para>To get an impression what these numbers actually mean, we will create a baseline. A
            baseline is a dumb model that indicates (more or less) the range we are working in. Our
            basline will simply pick the most frequent tag for every token (as in, most frequent in
            the corpus, not for the token). We will generalize the function a bit, allowing us to
            specify the tag to be
            used:<programlisting>baselineTagger :: Tag -> Token -> Maybe Tag
baselineTagger tag _ = Just tag</programlisting></para>
        <para>The most frequent tag in the Brown corpus is <emphasis role="italic">NN</emphasis>,
            the singular common noun. Let's evaluate the baseline
            tagger:<screen>*Main> <userinput>h &lt;- IO.openFile "brown-pos-test.txt" IO.ReadMode</userinput>
*Main> <userinput>c &lt;- IO.hGetContents h</userinput>
*Main> <userinput>evalTagger (baselineTagger "NN") $ map toTrainingInstance $ words c</userinput>
(272901,31815,0)</screen></para>
        <para>We sure do a lot better than this baseline at <emphasis role="italic">31815 / 272901 *
                100% = 11.66%</emphasis>! What if we implement the same heuristic for unknown words
            in our frequency tagger? You may expect it to only correct a small proportion of unknown
            words, but trying never hurts. We add a function named
                <function>backOffTagger</function> that wraps a tagger, returning some default tag
            if the tagger failed to find a tag for a
            token:<programlisting>backoffTagger :: (Token -> Maybe Tag) -> Tag -> Token -> Maybe Tag
backoffTagger f bt t = let pick = f t in
                       case pick of
                         Just tag -> Just tag
                         Nothing  -> Just bt</programlisting></para>
        <para>See how we can nicely cascade taggers by writing higher-order functions? We proceed to
            evaluate this
            tagger:<screen>*Main> <userinput>h &lt;- IO.openFile "brown-pos-train.txt" IO.ReadMode</userinput>
*Main> <userinput>c &lt;- IO.hGetContents h</userinput>
*Main> <userinput>let model = trainFreqTagger $ map toTrainingInstance $ words c</userinput>
*Main> <userinput>i &lt;- IO.openFile "brown-pos-test.txt" IO.ReadMode</userinput>
*Main> <userinput>d &lt;- IO.hGetContents i</userinput>
*Main> <userinput>evalTagger (backoffTagger (freqTagWord model) "NN") $
  map toTrainingInstance $ words d</userinput>
(272901,241590,0)</screen></para>
        <para>That did improve performance some. Of the 11536 tokens that we did not tag in the
            frequency-based tagger, we now tagged 2360 tokens correctly. This means that we tagged
            20.46% of the unknown words correctly. This is almost double of the baseline, how is
            that possible? It turns out that of some classes of tokens, such as articles,
            prepositions, and tokens, you will never encounter new ones in unseen data. Unknown
            words are often nouns, verbs, and adjectives. Since nouns form a larger proposotion of
            unknown words than all words, you will also get a better performance when guessing that
            a word is a singular common noun in unseen data.</para>
        <para><xref linkend="tbl-tagging-freq-performance"/> summarizes the result so far. We have
            also added the score of the <emphasis role="italic">oracle</emphasis> this is the
            performance that you would attain if the tagger was omniscient. In this task, the
                <emphasis role="italic">oracle</emphasis> performs the task perfectly, but this is
            not true for every task.<table frame="all" xml:id="tbl-tagging-freq-performance">
                <title>Performance of the frequency tagger.</title>
                <tgroup cols="2">
                    <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                    <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                    <thead>
                        <row>
                            <entry>Tagger</entry>
                            <entry>Accuracy (%)</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>Baseline</entry>
                            <entry>11.66</entry>
                        </row>
                        <row>
                            <entry>Frequency-based</entry>
                            <entry>87.66</entry>
                        </row>
                        <row>
                            <entry>Frequency-based + backoff</entry>
                            <entry>88.53</entry>
                        </row>
                        <row>
                            <entry>Oracle</entry>
                            <entry>100.00</entry>
                        </row>
                    </tbody>
                </tgroup>
            </table></para>
    </sect1>
    <sect1 xml:id="sec-tagging-tbl">
        <title>Transformation-based tagging</title>
        <para>While the frequency-tagger that you developed over the last two sections was a good
            first attempt at POS tagging, the performance of taggers can be improved by taking
            context into account. To give an example, the token 'saving' is used as a verb most
            frequently. However, when the word is used as a noun, this can often be derived from the
            context, as in the following two sentences:<orderedlist>
                <listitem>
                    <para><emphasis role="italic">The/AT weight/NN advantage/NN ,/, plus/CC
                            greater/JJR durability/NN of/IN the/AT plastic/NN unit/NN ,/, yields/VBZ
                            a/AT <emphasis role="bold">saving/NN</emphasis> of/IN about/RB
                            one-fifth/NN in/IN shipping/VBG ./.</emphasis></para>
                </listitem>
                <listitem>
                    <para><emphasis role="italic">Its/PP$ elimination/NN would/MD result/VB in/IN
                            the/AT <emphasis role="bold">saving/NN</emphasis> of/IN interest/ NN
                            costs/NNS ,/, heavy/JJ when/WRB short-term/NN money/NN rates/NNS are/BER
                            high /JJ ,/, and/CC in/IN freedom/NN from/IN dependence/NN on/IN
                            credit/NN which/WDT is/BEZ not/* always/RB available/JJ when/WRB
                            needed/VBN most/RBT ./.</emphasis></para>
                </listitem>
            </orderedlist></para>
        <para>In both cases, saving is preceded by an article and succeeded by a preposition. The
            context disambiguates what specific reading of the token 'saving' should be used.</para>
        <para>We could manually inspect all errors in the training corpus after tagging it with the
            frequency-based tagger, and write rules that correct mistaggings. This has been done in
            the past, and can give a tremendous boost in performance. Unfortunately, finding such
            rules is very tedious work, and specific to one language and tag set. Fortunately, <xref
                linkend="bib-brill1992"/> has shown that such rules can be learnt automatically
            using so-called <emphasis role="italic">transformation-based learning</emphasis>. The
            learning produre is straightforward:<orderedlist>
                <listitem>
                    <para>Tag every token in the training corpus using the most frequent tag for a
                        word.</para>
                </listitem>
                <listitem>
                    <para>Create rules from rule templates that correct incorrectly tagged
                        words.</para>
                </listitem>
                <listitem>
                    <para>Count how many corrections were made and errors were introduced when each
                        rule is applied to the corpus.</para>
                </listitem>
                <listitem>
                    <para>Select the a rule according to the following equation:<equation>
                            <title>Transformation rule selection criterium</title>
                            <xi:include href="eq-tbl-rule-score.mml"/>
                        </equation></para>
                </listitem>
                <listitem>
                    <para>Go to step 2, unless a threshold has been reached (e.g. rules do not give
                        a net improvement).</para>
                </listitem>
            </orderedlist></para>
        <para>The rule templates follow a very simple format. These are two examples from Brill's paper:<orderedlist>
                <listitem>
                    <para><emphasis role="italic">old_tag new_tag NEXT-TAG tag</emphasis></para>
                </listitem>
                <listitem>
                    <para><emphasis role="italic">old_tag new_tag PREV-TAG to</emphasis></para>
                </listitem>
            </orderedlist></para>
        <para>Two possible rules derived from these rule templates are:<orderedlist>
                <listitem>
                    <para>TO IN NEXT-TAG AT</para>
                </listitem>
                <listitem>
                    <para>NN VB PREV-TAG TO</para>
                </listitem>
            </orderedlist></para>
        <para>The first rule replaces the tag 'TO' (infinitival 'to') by 'IN' (preposition) if the
            next tag is 'AT' (article). The second rule, replaces the tag 'NN' (singular common
            noun) to 'VB' (verb, base) if the previous tag was 'TO' (infinitival 'to'). As you can
            immediately see, these are two very effective rules.</para>
        <para>Since you already have a frequency-based tagger, you can already perform the first
            step of the learning procedure for transformation-based tagging. What we still need are
            rule templates, rule extractors, and a scoring function. For brevity, we will only focus
            on three tag-based templates, but after implementing the learning procedure, it should
            be fairly obvious how to had other contexts and integrating words in templates. The
            templates that we will create, will take be all variations on directly surrounding tags
            (previous tag, next tag, both surrounding tags). Thanks to Haskell's algabraic data
            types, we can easily model these
            templates:<programlisting>data Replacement = Replacement Tag Tag
                   deriving Show

data TransformationRule = 
      NextTagRule     Replacement Tag
    | PrevTagRule     Replacement Tag
    | SurroundTagRule Replacement Tag Tag
      deriving Show</programlisting></para>
        <para>To confirm that these templates are indeed working as expected, we can recreate the
            rules that were mentioned earlier:</para>
        <screen>*Main> <userinput>NextTagRule (Replacement "TO" "IN") "AT"</userinput>
NextTagRule (Replacement "TO" "IN") "AT"
*Main> <userinput>PrevTagRule (Replacement "NN" "VB") "TO"</userinput>
PrevTagRule (Replacement "NN" "VB") "TO"</screen>
        <para>Awesome! Now on to rule instantiation. We need to instantiate rules for tags that are
            incorrect, so ideally we have the corpus represented as a list of binary tuples, where
            the first element is the correct tag, and the second element the tag that is currently
            assigned by the tagger. For instance:</para>
        <programlisting>[("AT","AT"),("NN","VB"),("TO", "TO")]</programlisting>
        <para>This can simply be done by using Haskell's <function>zip</function> function, that
            'zips' together two lists into one list of binary
            tuples:<screen>*Main> <userinput>:type zip</userinput>
zip :: [a] -> [b] -> [(a, b)]
*Main> <userinput>let correct = ["AT","NN","TO"]</userinput>
*Main> <userinput>let tagged = ["AT","VB","TO"]</userinput>
*Main> <userinput>zip correct tagged</userinput>
[("AT","AT"),("NN","VB"),("TO","TO")]</screen></para>
        <para>However, using lists is not really practical in this case. By the way they are
            normally traversed, the current element is always the head, meaning that we do not
            readily have access to previous elements. But we no need to access previous elements for
            the <type>PrevTagRule</type> and <type>SurroundTagRule</type> templates. We can write
            our own function that keeps track of previous elements, but a package with such
            functionality, called <package>ListZipper</package>, is already available. After using
                <command>cabal</command> to install the <package>ListZipper</package> package, you
            will have access to the <type>Data.List.Zipper</type> module. A
                <type>Zipper</type> can be seen as a list that can be traversed in two directions.
            We can construct a <type>Zipper</type> from a
            list:<screen>*Main> <userinput>let taggingState = Data.List.Zipper.fromList $
  zip ["AT","NN","TO"]  ["AT","VB","TO"]</userinput>
*Main> <userinput>taggingState</userinput>
Zip [] [("AT","AT"),("NN","VB"),("TO","TO")]</screen></para>
        <para>We can get the current element (the element the so-called <emphasis role="italic"
                >cursor</emphasis> is pointing at) in the zipper using
                <function>Data.List.Zipper.cursor</function>:</para>
        <screen>*Main> <userinput>Data.List.Zipper.cursor taggingState</userinput>
("AT","AT")</screen>
        <para>We can move the cursor to the left (point to the previous element) with
                <function>Data.List.Zipper.left</function>, and to the right (point to the next
            element) with <function>Data.List.Zipper.right</function>:</para>
        <screen>*Main> <userinput>Data.List.Zipper.right taggingState</userinput>
Zip [("AT","AT")] [("NN","VB"),("TO","TO")]
*Main> <userinput>Data.List.Zipper.cursor $ Data.List.Zipper.right taggingState</userinput>
("NN","VB")
*Main> <userinput>Data.List.Zipper.left $ Data.List.Zipper.right $ taggingState</userinput>
Zip [] [("AT","AT"),("NN","VB"),("TO","TO")]
*Main> <userinput>Data.List.Zipper.cursor $ Data.List.Zipper.left $
  Data.List.Zipper.right $ taggingState</userinput>
("AT","AT")</screen>
        <para>This allows us to do the kind of maneuvering necessary to extract rules. The rule
            instantiations are modelled as functions, and are pretty simple: they just pick the
            information that is necessary out of their environment. We have to bit careful at the
            boundaries of the <type>Zipper</type> though: at the beginning of the
                <type>Zipper</type> only <type>NextTagRule</type> can extract the necessary
            information, and at the end of the <type>Zipper</type> this applies to
                <type>PrevTagRule</type>. To be able to handle such situations, we make the return
            type of the instantiation functions <type>Maybe TransformationRule</type>. Let's go
            through the instantiation functions one by one, starting with
                <function>instNextTagRule0</function> (we add the '0' suffix, since we will prettify
            these functions
            later):<programlisting>import qualified Data.List.Zipper as Z

instNextTagRule0 :: Z.Zipper (Tag, Tag) -> Maybe TransformationRule
instNextTagRule0 z
    | Z.endp z = Nothing
    | Z.endp $ Z.right z = Nothing
    | otherwise = Just $ NextTagRule (Replacement incorrectTag correctTag) nextTag
    where (correctTag, incorrectTag) = Z.cursor z
          nextTag = snd $ Z.cursor $ Z.right z</programlisting></para>
        <para>When instantiating a rule from the current element in the <type>Zipper</type>, we have
            two problematic conditions to check for. The first is that the <type>Zipper</type> does
            not point to an element. This happens when we would traverse to the right when are
            already at the last element of the <type>Zipper</type>. In the second condition, we are
            actually at the last element of the <type>Zipper</type>. In this situation, we cannot
            extract the next <type>Tag</type>. For both conditions, we return <type>Nothing</type>.
            When these conditions do not hold, we can extract a <type>NextTagRule</type>. We do this
            by defining the replacement, replacing the incorrect tag by the correct one, and
            extracting the next tag. We can test this instantiation function, assuming that
                <varname>taggingState</varname> is defined as
            above:<screen>*Main> <userinput>instNextTagRule0 $ Data.List.Zipper.right taggingState</userinput>
Just (NextTagRule (Replacement "VB" "NN") "TO")</screen></para>
        <para>The <function>instPrevTag0</function> function is almost similar, except that in the
            second condition returns <type>Nothing</type> if the current element is the first
            element of the <type>Zipper</type>. And, of course, we extract the previous tag rather
            than the next
            tag:<programlisting>instPrevTagRule0 :: Z.Zipper (Tag, Tag) -> Maybe TransformationRule
instPrevTagRule0 z
    | Z.endp z = Nothing
    | Z.beginp z = Nothing
    | otherwise = Just $ PrevTagRule (Replacement incorrectTag correctTag) prevTag
    where (correctTag, incorrectTag) = Z.cursor z
          prevTag = snd $ Z.cursor $ Z.left z</programlisting></para>
        <para>Let's do a sanity check to be
            safe:<screen>*Main> <userinput>instPrevTagRule0 $ Data.List.Zipper.right taggingState</userinput>
Just (PrevTagRule (Replacement "VB" "NN") "AT")</screen></para>
        <para>Finally, we write the <function>instSurroundTag0</function> function, which combines
            the functionality of <function>instNextTag0</function> and
                <function>instPrevTag0</function>:<programlisting>instSurroundTagRule0 :: Z.Zipper (Tag, Tag) -> Maybe TransformationRule
instSurroundTagRule0 z
    | Z.endp z = Nothing
    | Z.beginp z = Nothing
    | Z.endp $ Z.right z = Nothing
    | otherwise = Just $ SurroundTagRule (Replacement incorrectTag correctTag)
                  prevTag nextTag
    where (correctTag, incorrectTag) = Z.cursor z
          prevTag = snd $ Z.cursor $ Z.left z
          nextTag = snd $ Z.cursor $ Z.right z</programlisting></para>
        <para>And this also works as
            intended:<screen>*Main> <userinput>instSurroundTagRule0 $ Data.List.Zipper.right taggingState</userinput>
Just (SurroundTagRule (Replacement "VB" "NN") "AT" "TO")</screen></para>
        <para>We will make these functions simpler by making use of the <type>Maybe</type> monad.
            First, we define two functions to get the previous and the next element of the zipper,
            wrapped in <type>Maybe</type>. To accomplish this, we use the
                <function>safeCursor</function> function, which returns the element the cursor
            points at using <type>Maybe</type>. It will return value <type>Nothing</type> if the
            cursor points beyond the last element of the
            zipper.<programlisting>rightCursor :: Z.Zipper (Tag, Tag) -> Maybe (Tag, Tag)
rightCursor = Z.safeCursor . Z.right

leftCursor :: Z.Zipper (Tag, Tag) -> Maybe (Tag, Tag)
leftCursor z = if Z.beginp z then
                   Nothing
               else
                   Z.safeCursor $ Z.left z</programlisting></para>
        <para>The <function>rightCursor</function> function is trivial. The
                <function>leftCursor</function> is a bit more complicated, since calling
                <function>left</function> on a <type>Zipper</type> with a cursor pointing at the
            first element, will return an equivalent <type>Zipper</type>. So, we return
                <type>Nothing</type> when we are pointing at the first element (and cannot move
            left).</para>
        <para>In our previous implementations of the instantiation functions, we checked all failure
            conditions using guards. However, once we work with expressions evaluating to
                <type>Maybe</type>, we can use the <type>Maybe</type> monad instead. The
                <type>Maybe</type> monad represents computations that could fail (return
                <type>Nothing</type>), and a failure will be propagated (the monad will end in
                <type>Nothing</type>). The <function>return</function> function is used to pack the
            value of the final expression in a <type>Maybe</type>.</para>
        <para>Using the <type>Maybe</type> monad, we can simplify the instantiation
            functions:</para>
        <programlisting>instNextTagRule :: Z.Zipper (Tag, Tag) -> Maybe TransformationRule
instNextTagRule z = do
    (_, next) &lt;- rightCursor z
    (correct, incorrect) &lt;- Z.safeCursor z
    return $ NextTagRule (Replacement incorrect correct) next

instPrevTagRule :: Z.Zipper (Tag, Tag) -> Maybe TransformationRule
instPrevTagRule z = do
    (_, prev)            &lt;- leftCursor z
    (correct, incorrect) &lt;- Z.safeCursor z
    return $ PrevTagRule (Replacement incorrect correct) prev


instSurroundTagRule :: Z.Zipper (Tag, Tag) -> Maybe TransformationRule
instSurroundTagRule z = do
    (_, next)            &lt;- rightCursor z
    (_, prev)            &lt;- leftCursor z
    (correct, incorrect) &lt;- Z.safeCursor z
    return $ SurroundTagRule (Replacement incorrect correct) prev next</programlisting>
    </sect1>
    <bibliography>
        <biblioentry xml:id="bib-brill1992">
            <title>A simple rule-based part of speech tagger</title>
            <author>
                <personname>E. Brill</personname>
            </author>
            <date>1992</date>
            <publisher>
                <publishername>Association for Computational Linguistics</publishername>
            </publisher>
        </biblioentry>
    </bibliography>
</chapter>
